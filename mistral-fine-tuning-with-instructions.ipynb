{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6ff7b1d",
   "metadata": {},
   "source": [
    "# Fine-tuning Mistral-7B for Instruction Generation\n",
    "\n",
    "## Overview\n",
    "\n",
    "This Jupyter notebook demonstrates the process of fine-tuning the Mistral-7B language model for instruction generation using Parameter-Efficient Fine-Tuning (PEFT) with Low-Rank Adaptation (LoRA). The goal is to adapt the model to generate instructions based on given inputs and responses, essentially reversing the typical instruction-following behavior of large language models.\n",
    "\n",
    "## Purpose\n",
    "\n",
    "- Showcase the fine-tuning process for large language models\n",
    "- Demonstrate the use of LoRA for efficient adaptation of pre-trained models\n",
    "- Provide a practical example of preparing data, configuring models, and training for a specific NLP task\n",
    "\n",
    "## Key Components\n",
    "\n",
    "1. Data preparation using the mosaicml/instruct-v3 dataset\n",
    "2. Model loading and configuration with 4-bit quantization\n",
    "3. LoRA setup for parameter-efficient fine-tuning\n",
    "4. Training process using the SFTTrainer from the TRL library\n",
    "\n",
    "## How to Use This Notebook\n",
    "\n",
    "1. **Environment Setup**: Ensure you have a GPU-enabled environment with Python and Jupyter installed.\n",
    "2. **Dependencies**: Run the first cell to install required libraries.\n",
    "3. **Data Preparation**: Follow the cells that load and preprocess the dataset.\n",
    "4. **Model Configuration**: Execute cells that load and configure the Mistral-7B model.\n",
    "5. **Training**: Run the training cell to fine-tune the model.\n",
    "6. **Evaluation**: Use the provided functions to test the model's performance after training.\n",
    "\n",
    "## Notes\n",
    "\n",
    "- This notebook uses a subset of the full dataset for quicker experimentation. Adjust dataset size as needed.\n",
    "- The training process is resource-intensive. Ensure you have adequate GPU memory available.\n",
    "- Experiment with different LoRA configurations and training parameters to optimize results.\n",
    "\n",
    "By following this notebook, you'll gain hands-on experience in fine-tuning large language models for specific tasks using state-of-the-art techniques in natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40505600",
   "metadata": {},
   "source": [
    "## Installing Required Libraries\n",
    "\n",
    "**What it's doing:**\n",
    "Installing necessary Python libraries for the project.\n",
    "\n",
    "**Why:**\n",
    "These libraries are essential for working with transformers, fine-tuning models, handling datasets, and optimizing performance. Installing them ensures we have all the tools needed for our task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db2c5907-8fd0-47c2-bbf0-12a4d26471bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install transformers trl accelerate torch bitsandbytes peft datasets -qU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa936223",
   "metadata": {},
   "source": [
    "## Loading the Dataset\n",
    "\n",
    "**What it's doing:**\n",
    "Loading the \"mosaicml/instruct-v3\" dataset.\n",
    "\n",
    "**Why:**\n",
    "This dataset contains instruction-response pairs, which are crucial for our task of fine-tuning a model to generate instructions. It provides the training data we need.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c5d2739-a164-4393-8e49-7abcaa434eec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "instruct_tune_dataset = load_dataset(\"mosaicml/instruct-v3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec9c35c",
   "metadata": {},
   "source": [
    "## Examining the Dataset\n",
    "\n",
    "**What it's doing:**\n",
    "Displaying the structure of the loaded dataset.\n",
    "\n",
    "**Why:**\n",
    "This helps us understand the composition of our dataset, including the number of examples and the available features. It's an important step for data exploration and verification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69a89a17-9001-452a-895a-5c3e5203ee13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'response', 'source'],\n",
       "        num_rows: 56167\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt', 'response', 'source'],\n",
       "        num_rows: 6807\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruct_tune_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a568007",
   "metadata": {},
   "source": [
    "## Filtering the Dataset\n",
    "\n",
    "**What it's doing:**\n",
    "Filtering the dataset to only include examples from the \"dolly_hhrlhf\" source.\n",
    "\n",
    "**Why:**\n",
    "By focusing on a specific subset of the data, we can potentially improve the quality and consistency of our fine-tuning results. This step helps in data curation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c320919-dfc2-47c6-9d51-770640391775",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'response', 'source'],\n",
       "        num_rows: 34333\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt', 'response', 'source'],\n",
       "        num_rows: 4771\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruct_tune_dataset = instruct_tune_dataset.filter(lambda x: x[\"source\"] == \"dolly_hhrlhf\")\n",
    "instruct_tune_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fac3e9f",
   "metadata": {},
   "source": [
    "## Reducing Dataset Size\n",
    "\n",
    "**What it's doing:**\n",
    "Limiting the dataset to 5,000 training examples and 200 test examples.\n",
    "\n",
    "**Why:**\n",
    "This reduction in dataset size allows for faster experimentation and requires less computational resources. It's a common practice when initially developing and testing a model fine-tuning pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fd23da7-e288-4e0e-b1f2-6c80e46d9080",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'response', 'source'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt', 'response', 'source'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruct_tune_dataset[\"train\"] = instruct_tune_dataset[\"train\"].select(range(5_000))\n",
    "instruct_tune_dataset[\"test\"] = instruct_tune_dataset[\"test\"].select(range(200))\n",
    "instruct_tune_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1915b3",
   "metadata": {},
   "source": [
    "## Defining the Prompt Template\n",
    "\n",
    "**What it's doing:**\n",
    "Creating a template for formatting our training data.\n",
    "\n",
    "**Why:**\n",
    "This template structures our input data consistently, telling the model how to interpret the input and what kind of output we expect. It's crucial for instruction-tuning tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88a8225b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"<s>### Instruction:\n",
    "Use the provided input to create an instruction that could have been used to generate the response with an LLM.\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\n",
    "{response}</s>\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec9029e",
   "metadata": {},
   "source": [
    "## Creating the Prompt Function\n",
    "\n",
    "**What it's doing:**\n",
    "Defining a function to format each sample from our dataset according to the prompt template.\n",
    "\n",
    "**Why:**\n",
    "This function prepares our data for training, ensuring each example is formatted consistently and correctly for our specific task of instruction generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a30068c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(sample):\n",
    "    input_text = sample[\"response\"]  # The 'response' from the dataset becomes the 'input' for our new task\n",
    "    response_text = sample[\"prompt\"].replace(\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction\\n\", \"\").strip()\n",
    "    \n",
    "    full_prompt = prompt_template.format(input=input_text, response=response_text)\n",
    "    \n",
    "    return full_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0c279e",
   "metadata": {},
   "source": [
    "## Testing the Prompt Function\n",
    "\n",
    "**What it's doing:**\n",
    "Applying the prompt function to a sample from the dataset.\n",
    "\n",
    "**Why:**\n",
    "This test ensures our prompt function is working correctly before we use it in training. It's a crucial verification step in our data preparation process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb7a552d-4e55-4043-8e24-ae09ea0c753d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>### Instruction:\\nUse the provided input to create an instruction that could have been used to generate the response with an LLM.\\n\\n### Input:\\nThere are more than 12,000 species of grass. The most common is Kentucky Bluegrass, because it grows quickly, easily, and is soft to the touch. Rygrass is shiny and bright green colored. Fescues are dark green and shiny. Bermuda grass is harder but can grow in drier soil.\\n\\n### Response:\\nWhat are different types of grass?\\n\\n### Response</s>'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_prompt(instruct_tune_dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e2d84f",
   "metadata": {},
   "source": [
    "## Loading the Pre-trained Model and Tokenizer\n",
    "\n",
    "**What it's doing:**\n",
    "Loading the Mistral-7B model and its tokenizer, with 4-bit quantization.\n",
    "\n",
    "**Why:**\n",
    "This step prepares our base model for fine-tuning. The 4-bit quantization allows us to work with this large model on more modest hardware by reducing its memory footprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad67bb92-1be9-466d-b095-e9504920db3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b5f47211dfd40c2ab41879a748070e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm()\n",
      "        (post_attention_layernorm): MistralRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    device_map='auto',\n",
    "    quantization_config=nf4_config,\n",
    "    use_cache=False\n",
    ")\n",
    "\n",
    "print(model)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c70e5e",
   "metadata": {},
   "source": [
    "## Defining the Generation Function\n",
    "\n",
    "**What it's doing:**\n",
    "Creating a function to generate responses using our model.\n",
    "\n",
    "**Why:**\n",
    "This function allows us to test our model's outputs at various stages of fine-tuning, helping us assess its performance and progress.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "903b2df4-3a3a-418e-8015-7d73f16d049f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_response(prompt, model):\n",
    "  encoded_input = tokenizer(prompt,  return_tensors=\"pt\", add_special_tokens=True)\n",
    "  model_inputs = encoded_input.to('cuda')\n",
    "\n",
    "  generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "  decoded_output = tokenizer.batch_decode(generated_ids)\n",
    "\n",
    "  return decoded_output[0].replace(prompt, \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e1bfb6",
   "metadata": {},
   "source": [
    "## Testing the Generation Function\n",
    "\n",
    "**What it's doing:**\n",
    "Generating a response with our base model before fine-tuning.\n",
    "\n",
    "**Why:**\n",
    "This provides a baseline to compare against after fine-tuning, helping us understand how much the model's performance improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53737251-3fd6-401f-a6e3-ee20fa47606e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> \\nTo become a trained medical professional, you need to follow several steps. First, you should obtain a college education by earning a four-year undergraduate degree and a four-year doctorate program. After that, you must complete a residency program. Once you have completed your education and residency, you need to get licensed. Finally, establish a practice in your field.</s>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_response(\"### Instruction:\\nUse the provided input to create an instruction that could have been used to generate the response with an LLM.\\n\\n### Input:\\nI think it depends a little on the individual, but there are a number of steps you’ll need to take.  First, you’ll need to get a college education.  This might include a four-year undergraduate degree and a four-year doctorate program.  You’ll also need to complete a residency program.  Once you have your education, you’ll need to be licensed.  And finally, you’ll need to establish a practice.\\n\\n### Response:\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46499063",
   "metadata": {},
   "source": [
    "## Configuring LoRA for Fine-tuning\n",
    "\n",
    "**What it's doing:**\n",
    "Setting up the Low-Rank Adaptation (LoRA) configuration for fine-tuning.\n",
    "\n",
    "**Why:**\n",
    "LoRA allows us to fine-tune the model efficiently by adding a small number of trainable parameters. This configuration defines how LoRA will be applied to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c53c0dc8-4447-4a8f-b3fc-b5b39d49740d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM, LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c045952",
   "metadata": {},
   "source": [
    "## Preparing the Model for LoRA Fine-tuning\n",
    "\n",
    "**What it's doing:**\n",
    "Applying the LoRA configuration to our model.\n",
    "\n",
    "**Why:**\n",
    "This step prepares our model for efficient fine-tuning, setting up the additional LoRA parameters while keeping most of the original model frozen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1c8904d-7e60-43ee-8495-f9abf3ba40de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='mistralai/Mistral-7B-Instruct-v0.1', revision=None, task_type='CAUSAL_LM', inference_mode=False, r=64, target_modules={'q_proj', 'v_proj'}, lora_alpha=16, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)\n",
      "Module: \n",
      "Module: base_model\n",
      "Module: base_model.model\n",
      "Module: base_model.model.model\n",
      "Module: base_model.model.model.embed_tokens\n",
      "Module: base_model.model.model.layers\n",
      "Module: base_model.model.model.layers.0\n",
      "Module: base_model.model.model.layers.0.self_attn\n",
      "Module: base_model.model.model.layers.0.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.0.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.0.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.0.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.0.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.0.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.0.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.0.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.0.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.0.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.0.self_attn.q_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.0.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.0.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.0.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.0.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.0.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.0.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.0.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.0.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.0.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.0.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.0.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.0.self_attn.v_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.0.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.0.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.0.mlp\n",
      "Module: base_model.model.model.layers.0.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.0.mlp.up_proj\n",
      "Module: base_model.model.model.layers.0.mlp.down_proj\n",
      "Module: base_model.model.model.layers.0.mlp.act_fn\n",
      "Module: base_model.model.model.layers.0.input_layernorm\n",
      "Module: base_model.model.model.layers.0.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.1\n",
      "Module: base_model.model.model.layers.1.self_attn\n",
      "Module: base_model.model.model.layers.1.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.1.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.1.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.1.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.1.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.1.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.1.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.1.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.1.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.1.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.1.self_attn.q_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.1.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.1.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.1.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.1.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.1.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.1.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.1.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.1.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.1.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.1.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.1.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.1.self_attn.v_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.1.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.1.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.1.mlp\n",
      "Module: base_model.model.model.layers.1.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.1.mlp.up_proj\n",
      "Module: base_model.model.model.layers.1.mlp.down_proj\n",
      "Module: base_model.model.model.layers.1.mlp.act_fn\n",
      "Module: base_model.model.model.layers.1.input_layernorm\n",
      "Module: base_model.model.model.layers.1.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.2\n",
      "Module: base_model.model.model.layers.2.self_attn\n",
      "Module: base_model.model.model.layers.2.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.2.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.2.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.2.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.2.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.2.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.2.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.2.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.2.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.2.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.2.self_attn.q_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.2.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.2.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.2.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.2.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.2.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.2.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.2.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.2.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.2.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.2.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.2.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.2.self_attn.v_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.2.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.2.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.2.mlp\n",
      "Module: base_model.model.model.layers.2.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.2.mlp.up_proj\n",
      "Module: base_model.model.model.layers.2.mlp.down_proj\n",
      "Module: base_model.model.model.layers.2.mlp.act_fn\n",
      "Module: base_model.model.model.layers.2.input_layernorm\n",
      "Module: base_model.model.model.layers.2.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.3\n",
      "Module: base_model.model.model.layers.3.self_attn\n",
      "Module: base_model.model.model.layers.3.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.3.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.3.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.3.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.3.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.3.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.3.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.3.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.3.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.3.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.3.self_attn.q_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.3.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.3.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.3.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.3.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.3.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.3.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.3.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.3.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.3.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.3.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.3.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.3.self_attn.v_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.3.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.3.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.3.mlp\n",
      "Module: base_model.model.model.layers.3.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.3.mlp.up_proj\n",
      "Module: base_model.model.model.layers.3.mlp.down_proj\n",
      "Module: base_model.model.model.layers.3.mlp.act_fn\n",
      "Module: base_model.model.model.layers.3.input_layernorm\n",
      "Module: base_model.model.model.layers.3.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.4\n",
      "Module: base_model.model.model.layers.4.self_attn\n",
      "Module: base_model.model.model.layers.4.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.4.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.4.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.4.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.4.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.4.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.4.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.4.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.4.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.4.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.4.self_attn.q_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.4.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.4.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.4.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.4.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.4.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.4.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.4.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.4.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.4.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.4.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.4.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.4.self_attn.v_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.4.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.4.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.4.mlp\n",
      "Module: base_model.model.model.layers.4.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.4.mlp.up_proj\n",
      "Module: base_model.model.model.layers.4.mlp.down_proj\n",
      "Module: base_model.model.model.layers.4.mlp.act_fn\n",
      "Module: base_model.model.model.layers.4.input_layernorm\n",
      "Module: base_model.model.model.layers.4.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.5\n",
      "Module: base_model.model.model.layers.5.self_attn\n",
      "Module: base_model.model.model.layers.5.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.5.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.5.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.5.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.5.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.5.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.5.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.5.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.5.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.5.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.5.self_attn.q_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.5.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.5.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.5.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.5.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.5.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.5.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.5.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.5.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.5.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.5.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.5.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.5.self_attn.v_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.5.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.5.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.5.mlp\n",
      "Module: base_model.model.model.layers.5.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.5.mlp.up_proj\n",
      "Module: base_model.model.model.layers.5.mlp.down_proj\n",
      "Module: base_model.model.model.layers.5.mlp.act_fn\n",
      "Module: base_model.model.model.layers.5.input_layernorm\n",
      "Module: base_model.model.model.layers.5.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.6\n",
      "Module: base_model.model.model.layers.6.self_attn\n",
      "Module: base_model.model.model.layers.6.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.6.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.6.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.6.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.6.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.6.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.6.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.6.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.6.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.6.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.6.self_attn.q_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.6.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.6.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.6.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.6.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.6.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.6.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.6.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.6.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.6.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.6.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.6.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.6.self_attn.v_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.6.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.6.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.6.mlp\n",
      "Module: base_model.model.model.layers.6.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.6.mlp.up_proj\n",
      "Module: base_model.model.model.layers.6.mlp.down_proj\n",
      "Module: base_model.model.model.layers.6.mlp.act_fn\n",
      "Module: base_model.model.model.layers.6.input_layernorm\n",
      "Module: base_model.model.model.layers.6.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.7\n",
      "Module: base_model.model.model.layers.7.self_attn\n",
      "Module: base_model.model.model.layers.7.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.7.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.7.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.7.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.7.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.7.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.7.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.7.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.7.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.7.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.7.self_attn.q_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.7.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.7.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.7.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.7.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.7.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.7.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.7.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.7.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.7.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.7.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.7.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.7.self_attn.v_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.7.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.7.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.7.mlp\n",
      "Module: base_model.model.model.layers.7.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.7.mlp.up_proj\n",
      "Module: base_model.model.model.layers.7.mlp.down_proj\n",
      "Module: base_model.model.model.layers.7.mlp.act_fn\n",
      "Module: base_model.model.model.layers.7.input_layernorm\n",
      "Module: base_model.model.model.layers.7.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.8\n",
      "Module: base_model.model.model.layers.8.self_attn\n",
      "Module: base_model.model.model.layers.8.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.8.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.8.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.8.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.8.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.8.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.8.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.8.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.8.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.8.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.8.self_attn.q_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.8.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.8.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.8.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.8.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.8.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.8.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.8.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.8.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.8.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.8.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.8.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.8.self_attn.v_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.8.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.8.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.8.mlp\n",
      "Module: base_model.model.model.layers.8.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.8.mlp.up_proj\n",
      "Module: base_model.model.model.layers.8.mlp.down_proj\n",
      "Module: base_model.model.model.layers.8.mlp.act_fn\n",
      "Module: base_model.model.model.layers.8.input_layernorm\n",
      "Module: base_model.model.model.layers.8.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.9\n",
      "Module: base_model.model.model.layers.9.self_attn\n",
      "Module: base_model.model.model.layers.9.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.9.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.9.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.9.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.9.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.9.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.9.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.9.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.9.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.9.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.9.self_attn.q_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.9.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.9.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.9.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.9.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.9.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.9.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.9.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.9.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.9.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.9.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.9.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.9.self_attn.v_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.9.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.9.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.9.mlp\n",
      "Module: base_model.model.model.layers.9.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.9.mlp.up_proj\n",
      "Module: base_model.model.model.layers.9.mlp.down_proj\n",
      "Module: base_model.model.model.layers.9.mlp.act_fn\n",
      "Module: base_model.model.model.layers.9.input_layernorm\n",
      "Module: base_model.model.model.layers.9.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.10\n",
      "Module: base_model.model.model.layers.10.self_attn\n",
      "Module: base_model.model.model.layers.10.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.10.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.10.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.10.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.10.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.10.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.10.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.10.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.10.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.10.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.10.self_attn.q_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.10.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.10.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.10.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.10.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.10.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.10.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.10.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.10.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.10.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.10.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.10.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.10.self_attn.v_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.10.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.10.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.10.mlp\n",
      "Module: base_model.model.model.layers.10.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.10.mlp.up_proj\n",
      "Module: base_model.model.model.layers.10.mlp.down_proj\n",
      "Module: base_model.model.model.layers.10.mlp.act_fn\n",
      "Module: base_model.model.model.layers.10.input_layernorm\n",
      "Module: base_model.model.model.layers.10.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.11\n",
      "Module: base_model.model.model.layers.11.self_attn\n",
      "Module: base_model.model.model.layers.11.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.11.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.11.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.11.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.11.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.11.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.11.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.11.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.11.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.11.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.11.self_attn.q_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.11.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.11.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.11.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.11.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.11.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.11.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.11.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.11.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.11.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.11.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.11.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.11.self_attn.v_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.11.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.11.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.11.mlp\n",
      "Module: base_model.model.model.layers.11.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.11.mlp.up_proj\n",
      "Module: base_model.model.model.layers.11.mlp.down_proj\n",
      "Module: base_model.model.model.layers.11.mlp.act_fn\n",
      "Module: base_model.model.model.layers.11.input_layernorm\n",
      "Module: base_model.model.model.layers.11.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.12\n",
      "Module: base_model.model.model.layers.12.self_attn\n",
      "Module: base_model.model.model.layers.12.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.12.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.12.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.12.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.12.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.12.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.12.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.12.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.12.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.12.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.12.self_attn.q_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.12.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.12.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.12.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.12.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.12.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.12.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.12.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.12.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.12.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.12.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.12.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.12.self_attn.v_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.12.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.12.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.12.mlp\n",
      "Module: base_model.model.model.layers.12.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.12.mlp.up_proj\n",
      "Module: base_model.model.model.layers.12.mlp.down_proj\n",
      "Module: base_model.model.model.layers.12.mlp.act_fn\n",
      "Module: base_model.model.model.layers.12.input_layernorm\n",
      "Module: base_model.model.model.layers.12.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.13\n",
      "Module: base_model.model.model.layers.13.self_attn\n",
      "Module: base_model.model.model.layers.13.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.13.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.13.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.13.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.13.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.13.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.13.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.13.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.13.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.13.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.13.self_attn.q_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.13.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.13.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.13.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.13.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.13.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.13.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.13.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.13.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.13.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.13.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.13.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.13.self_attn.v_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.13.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.13.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.13.mlp\n",
      "Module: base_model.model.model.layers.13.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.13.mlp.up_proj\n",
      "Module: base_model.model.model.layers.13.mlp.down_proj\n",
      "Module: base_model.model.model.layers.13.mlp.act_fn\n",
      "Module: base_model.model.model.layers.13.input_layernorm\n",
      "Module: base_model.model.model.layers.13.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.14\n",
      "Module: base_model.model.model.layers.14.self_attn\n",
      "Module: base_model.model.model.layers.14.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.14.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.14.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.14.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.14.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.14.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.14.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.14.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.14.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.14.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.14.self_attn.q_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.14.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.14.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.14.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.14.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.14.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.14.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.14.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.14.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.14.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.14.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.14.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.14.self_attn.v_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.14.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.14.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.14.mlp\n",
      "Module: base_model.model.model.layers.14.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.14.mlp.up_proj\n",
      "Module: base_model.model.model.layers.14.mlp.down_proj\n",
      "Module: base_model.model.model.layers.14.mlp.act_fn\n",
      "Module: base_model.model.model.layers.14.input_layernorm\n",
      "Module: base_model.model.model.layers.14.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.15\n",
      "Module: base_model.model.model.layers.15.self_attn\n",
      "Module: base_model.model.model.layers.15.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.15.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.15.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.15.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.15.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.15.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.15.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.15.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.15.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.15.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.15.self_attn.q_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.15.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.15.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.15.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.15.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.15.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.15.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.15.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.15.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.15.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.15.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.15.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.15.self_attn.v_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.15.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.15.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.15.mlp\n",
      "Module: base_model.model.model.layers.15.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.15.mlp.up_proj\n",
      "Module: base_model.model.model.layers.15.mlp.down_proj\n",
      "Module: base_model.model.model.layers.15.mlp.act_fn\n",
      "Module: base_model.model.model.layers.15.input_layernorm\n",
      "Module: base_model.model.model.layers.15.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.16\n",
      "Module: base_model.model.model.layers.16.self_attn\n",
      "Module: base_model.model.model.layers.16.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.16.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.16.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.16.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.16.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.16.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.16.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.16.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.16.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.16.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.16.self_attn.q_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.16.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.16.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.16.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.16.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.16.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.16.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.16.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.16.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.16.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.16.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.16.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.16.self_attn.v_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.16.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.16.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.16.mlp\n",
      "Module: base_model.model.model.layers.16.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.16.mlp.up_proj\n",
      "Module: base_model.model.model.layers.16.mlp.down_proj\n",
      "Module: base_model.model.model.layers.16.mlp.act_fn\n",
      "Module: base_model.model.model.layers.16.input_layernorm\n",
      "Module: base_model.model.model.layers.16.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.17\n",
      "Module: base_model.model.model.layers.17.self_attn\n",
      "Module: base_model.model.model.layers.17.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.17.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.17.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.17.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.17.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.17.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.17.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.17.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.17.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.17.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.17.self_attn.q_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.17.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.17.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.17.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.17.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.17.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.17.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.17.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.17.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.17.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.17.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.17.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.17.self_attn.v_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.17.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.17.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.17.mlp\n",
      "Module: base_model.model.model.layers.17.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.17.mlp.up_proj\n",
      "Module: base_model.model.model.layers.17.mlp.down_proj\n",
      "Module: base_model.model.model.layers.17.mlp.act_fn\n",
      "Module: base_model.model.model.layers.17.input_layernorm\n",
      "Module: base_model.model.model.layers.17.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.18\n",
      "Module: base_model.model.model.layers.18.self_attn\n",
      "Module: base_model.model.model.layers.18.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.18.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.18.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.18.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.18.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.18.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.18.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.18.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.18.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.18.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.18.self_attn.q_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.18.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.18.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.18.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.18.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.18.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.18.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.18.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.18.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.18.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.18.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.18.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.18.self_attn.v_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.18.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.18.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.18.mlp\n",
      "Module: base_model.model.model.layers.18.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.18.mlp.up_proj\n",
      "Module: base_model.model.model.layers.18.mlp.down_proj\n",
      "Module: base_model.model.model.layers.18.mlp.act_fn\n",
      "Module: base_model.model.model.layers.18.input_layernorm\n",
      "Module: base_model.model.model.layers.18.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.19\n",
      "Module: base_model.model.model.layers.19.self_attn\n",
      "Module: base_model.model.model.layers.19.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.19.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.19.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.19.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.19.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.19.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.19.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.19.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.19.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.19.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.19.self_attn.q_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.19.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.19.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.19.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.19.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.19.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.19.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.19.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.19.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.19.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.19.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.19.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.19.self_attn.v_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.19.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.19.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.19.mlp\n",
      "Module: base_model.model.model.layers.19.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.19.mlp.up_proj\n",
      "Module: base_model.model.model.layers.19.mlp.down_proj\n",
      "Module: base_model.model.model.layers.19.mlp.act_fn\n",
      "Module: base_model.model.model.layers.19.input_layernorm\n",
      "Module: base_model.model.model.layers.19.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.20\n",
      "Module: base_model.model.model.layers.20.self_attn\n",
      "Module: base_model.model.model.layers.20.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.20.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.20.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.20.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.20.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.20.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.20.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.20.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.20.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.20.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.20.self_attn.q_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.20.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.20.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.20.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.20.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.20.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.20.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.20.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.20.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.20.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.20.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.20.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.20.self_attn.v_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.20.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.20.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.20.mlp\n",
      "Module: base_model.model.model.layers.20.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.20.mlp.up_proj\n",
      "Module: base_model.model.model.layers.20.mlp.down_proj\n",
      "Module: base_model.model.model.layers.20.mlp.act_fn\n",
      "Module: base_model.model.model.layers.20.input_layernorm\n",
      "Module: base_model.model.model.layers.20.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.21\n",
      "Module: base_model.model.model.layers.21.self_attn\n",
      "Module: base_model.model.model.layers.21.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.21.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.21.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.21.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.21.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.21.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.21.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.21.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.21.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.21.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.21.self_attn.q_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.21.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.21.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.21.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.21.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.21.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.21.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.21.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.21.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.21.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.21.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.21.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.21.self_attn.v_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.21.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.21.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.21.mlp\n",
      "Module: base_model.model.model.layers.21.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.21.mlp.up_proj\n",
      "Module: base_model.model.model.layers.21.mlp.down_proj\n",
      "Module: base_model.model.model.layers.21.mlp.act_fn\n",
      "Module: base_model.model.model.layers.21.input_layernorm\n",
      "Module: base_model.model.model.layers.21.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.22\n",
      "Module: base_model.model.model.layers.22.self_attn\n",
      "Module: base_model.model.model.layers.22.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.22.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.22.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.22.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.22.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.22.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.22.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.22.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.22.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.22.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.22.self_attn.q_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.22.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.22.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.22.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.22.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.22.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.22.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.22.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.22.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.22.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.22.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.22.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.22.self_attn.v_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.22.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.22.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.22.mlp\n",
      "Module: base_model.model.model.layers.22.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.22.mlp.up_proj\n",
      "Module: base_model.model.model.layers.22.mlp.down_proj\n",
      "Module: base_model.model.model.layers.22.mlp.act_fn\n",
      "Module: base_model.model.model.layers.22.input_layernorm\n",
      "Module: base_model.model.model.layers.22.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.23\n",
      "Module: base_model.model.model.layers.23.self_attn\n",
      "Module: base_model.model.model.layers.23.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.23.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.23.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.23.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.23.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.23.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.23.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.23.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.23.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.23.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.23.self_attn.q_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.23.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.23.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.23.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.23.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.23.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.23.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.23.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.23.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.23.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.23.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.23.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.23.self_attn.v_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.23.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.23.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.23.mlp\n",
      "Module: base_model.model.model.layers.23.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.23.mlp.up_proj\n",
      "Module: base_model.model.model.layers.23.mlp.down_proj\n",
      "Module: base_model.model.model.layers.23.mlp.act_fn\n",
      "Module: base_model.model.model.layers.23.input_layernorm\n",
      "Module: base_model.model.model.layers.23.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.24\n",
      "Module: base_model.model.model.layers.24.self_attn\n",
      "Module: base_model.model.model.layers.24.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.24.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.24.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.24.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.24.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.24.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.24.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.24.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.24.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.24.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.24.self_attn.q_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.24.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.24.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.24.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.24.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.24.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.24.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.24.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.24.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.24.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.24.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.24.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.24.self_attn.v_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.24.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.24.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.24.mlp\n",
      "Module: base_model.model.model.layers.24.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.24.mlp.up_proj\n",
      "Module: base_model.model.model.layers.24.mlp.down_proj\n",
      "Module: base_model.model.model.layers.24.mlp.act_fn\n",
      "Module: base_model.model.model.layers.24.input_layernorm\n",
      "Module: base_model.model.model.layers.24.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.25\n",
      "Module: base_model.model.model.layers.25.self_attn\n",
      "Module: base_model.model.model.layers.25.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.25.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.25.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.25.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.25.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.25.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.25.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.25.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.25.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.25.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.25.self_attn.q_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.25.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.25.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.25.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.25.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.25.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.25.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.25.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.25.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.25.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.25.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.25.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.25.self_attn.v_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.25.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.25.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.25.mlp\n",
      "Module: base_model.model.model.layers.25.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.25.mlp.up_proj\n",
      "Module: base_model.model.model.layers.25.mlp.down_proj\n",
      "Module: base_model.model.model.layers.25.mlp.act_fn\n",
      "Module: base_model.model.model.layers.25.input_layernorm\n",
      "Module: base_model.model.model.layers.25.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.26\n",
      "Module: base_model.model.model.layers.26.self_attn\n",
      "Module: base_model.model.model.layers.26.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.26.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.26.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.26.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.26.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.26.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.26.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.26.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.26.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.26.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.26.self_attn.q_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.26.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.26.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.26.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.26.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.26.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.26.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.26.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.26.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.26.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.26.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.26.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.26.self_attn.v_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.26.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.26.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.26.mlp\n",
      "Module: base_model.model.model.layers.26.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.26.mlp.up_proj\n",
      "Module: base_model.model.model.layers.26.mlp.down_proj\n",
      "Module: base_model.model.model.layers.26.mlp.act_fn\n",
      "Module: base_model.model.model.layers.26.input_layernorm\n",
      "Module: base_model.model.model.layers.26.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.27\n",
      "Module: base_model.model.model.layers.27.self_attn\n",
      "Module: base_model.model.model.layers.27.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.27.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.27.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.27.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.27.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.27.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.27.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.27.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.27.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.27.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.27.self_attn.q_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.27.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.27.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.27.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.27.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.27.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.27.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.27.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.27.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.27.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.27.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.27.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.27.self_attn.v_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.27.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.27.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.27.mlp\n",
      "Module: base_model.model.model.layers.27.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.27.mlp.up_proj\n",
      "Module: base_model.model.model.layers.27.mlp.down_proj\n",
      "Module: base_model.model.model.layers.27.mlp.act_fn\n",
      "Module: base_model.model.model.layers.27.input_layernorm\n",
      "Module: base_model.model.model.layers.27.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.28\n",
      "Module: base_model.model.model.layers.28.self_attn\n",
      "Module: base_model.model.model.layers.28.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.28.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.28.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.28.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.28.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.28.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.28.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.28.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.28.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.28.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.28.self_attn.q_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.28.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.28.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.28.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.28.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.28.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.28.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.28.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.28.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.28.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.28.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.28.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.28.self_attn.v_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.28.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.28.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.28.mlp\n",
      "Module: base_model.model.model.layers.28.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.28.mlp.up_proj\n",
      "Module: base_model.model.model.layers.28.mlp.down_proj\n",
      "Module: base_model.model.model.layers.28.mlp.act_fn\n",
      "Module: base_model.model.model.layers.28.input_layernorm\n",
      "Module: base_model.model.model.layers.28.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.29\n",
      "Module: base_model.model.model.layers.29.self_attn\n",
      "Module: base_model.model.model.layers.29.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.29.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.29.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.29.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.29.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.29.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.29.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.29.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.29.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.29.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.29.self_attn.q_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.29.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.29.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.29.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.29.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.29.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.29.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.29.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.29.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.29.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.29.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.29.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.29.self_attn.v_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.29.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.29.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.29.mlp\n",
      "Module: base_model.model.model.layers.29.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.29.mlp.up_proj\n",
      "Module: base_model.model.model.layers.29.mlp.down_proj\n",
      "Module: base_model.model.model.layers.29.mlp.act_fn\n",
      "Module: base_model.model.model.layers.29.input_layernorm\n",
      "Module: base_model.model.model.layers.29.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.30\n",
      "Module: base_model.model.model.layers.30.self_attn\n",
      "Module: base_model.model.model.layers.30.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.30.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.30.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.30.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.30.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.30.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.30.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.30.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.30.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.30.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.30.self_attn.q_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.30.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.30.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.30.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.30.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.30.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.30.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.30.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.30.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.30.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.30.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.30.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.30.self_attn.v_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.30.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.30.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.30.mlp\n",
      "Module: base_model.model.model.layers.30.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.30.mlp.up_proj\n",
      "Module: base_model.model.model.layers.30.mlp.down_proj\n",
      "Module: base_model.model.model.layers.30.mlp.act_fn\n",
      "Module: base_model.model.model.layers.30.input_layernorm\n",
      "Module: base_model.model.model.layers.30.post_attention_layernorm\n",
      "Module: base_model.model.model.layers.31\n",
      "Module: base_model.model.model.layers.31.self_attn\n",
      "Module: base_model.model.model.layers.31.self_attn.q_proj\n",
      "Module: base_model.model.model.layers.31.self_attn.q_proj.base_layer\n",
      "Module: base_model.model.model.layers.31.self_attn.q_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.31.self_attn.q_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.31.self_attn.q_proj.lora_A\n",
      "Module: base_model.model.model.layers.31.self_attn.q_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.31.self_attn.q_proj.lora_B\n",
      "Module: base_model.model.model.layers.31.self_attn.q_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.31.self_attn.q_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.31.self_attn.q_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.31.self_attn.q_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.31.self_attn.k_proj\n",
      "Module: base_model.model.model.layers.31.self_attn.v_proj\n",
      "Module: base_model.model.model.layers.31.self_attn.v_proj.base_layer\n",
      "Module: base_model.model.model.layers.31.self_attn.v_proj.lora_dropout\n",
      "Module: base_model.model.model.layers.31.self_attn.v_proj.lora_dropout.default\n",
      "Module: base_model.model.model.layers.31.self_attn.v_proj.lora_A\n",
      "Module: base_model.model.model.layers.31.self_attn.v_proj.lora_A.default\n",
      "Module: base_model.model.model.layers.31.self_attn.v_proj.lora_B\n",
      "Module: base_model.model.model.layers.31.self_attn.v_proj.lora_B.default\n",
      "Module: base_model.model.model.layers.31.self_attn.v_proj.lora_embedding_A\n",
      "Module: base_model.model.model.layers.31.self_attn.v_proj.lora_embedding_B\n",
      "Module: base_model.model.model.layers.31.self_attn.v_proj.lora_magnitude_vector\n",
      "Module: base_model.model.model.layers.31.self_attn.o_proj\n",
      "Module: base_model.model.model.layers.31.self_attn.rotary_emb\n",
      "Module: base_model.model.model.layers.31.mlp\n",
      "Module: base_model.model.model.layers.31.mlp.gate_proj\n",
      "Module: base_model.model.model.layers.31.mlp.up_proj\n",
      "Module: base_model.model.model.layers.31.mlp.down_proj\n",
      "Module: base_model.model.model.layers.31.mlp.act_fn\n",
      "Module: base_model.model.model.layers.31.input_layernorm\n",
      "Module: base_model.model.model.layers.31.post_attention_layernorm\n",
      "Module: base_model.model.model.norm\n",
      "Module: base_model.model.lm_head\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.0.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.0.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.0.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.0.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.0.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.0.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.0.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.0.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.0.self_attn.q_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.0.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.0.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.0.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.0.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.0.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.0.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.0.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.0.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.0.self_attn.v_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.1.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.1.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.1.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.1.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.1.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.1.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.1.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.1.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.1.self_attn.q_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.1.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.1.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.1.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.1.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.1.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.1.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.1.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.1.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.1.self_attn.v_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.2.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.2.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.2.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.2.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.2.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.2.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.2.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.2.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.2.self_attn.q_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.2.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.2.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.2.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.2.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.2.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.2.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.2.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.2.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.2.self_attn.v_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.3.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.3.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.3.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.3.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.3.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.3.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.3.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.3.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.3.self_attn.q_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.3.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.3.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.3.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.3.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.3.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.3.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.3.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.3.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.3.self_attn.v_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.4.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.4.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.4.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.4.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.4.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.4.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.4.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.4.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.4.self_attn.q_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.4.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.4.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.4.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.4.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.4.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.4.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.4.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.4.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.4.self_attn.v_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.5.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.5.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.5.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.5.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.5.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.5.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.5.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.5.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.5.self_attn.q_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.5.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.5.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.5.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.5.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.5.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.5.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.5.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.5.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.5.self_attn.v_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.6.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.6.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.6.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.6.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.6.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.6.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.6.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.6.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.6.self_attn.q_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.6.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.6.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.6.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.6.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.6.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.6.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.6.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.6.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.6.self_attn.v_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.7.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.7.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.7.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.7.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.7.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.7.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.7.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.7.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.7.self_attn.q_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.7.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.7.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.7.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.7.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.7.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.7.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.7.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.7.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.7.self_attn.v_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.8.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.8.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.8.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.8.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.8.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.8.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.8.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.8.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.8.self_attn.q_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.8.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.8.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.8.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.8.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.8.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.8.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.8.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.8.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.8.self_attn.v_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.9.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.9.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.9.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.9.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.9.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.9.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.9.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.9.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.9.self_attn.q_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.9.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.9.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.9.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.9.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.9.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.9.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.9.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.9.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.9.self_attn.v_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.10.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.10.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.10.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.10.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.10.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.10.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.10.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.10.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.10.self_attn.q_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.10.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.10.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.10.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.10.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.10.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.10.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.10.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.10.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.10.self_attn.v_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.11.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.11.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.11.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.11.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.11.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.11.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.11.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.11.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.11.self_attn.q_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.11.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.11.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.11.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.11.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.11.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.11.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.11.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.11.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.11.self_attn.v_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.12.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.12.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.12.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.12.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.12.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.12.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.12.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.12.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.12.self_attn.q_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.12.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.12.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.12.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.12.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.12.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.12.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.12.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.12.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.12.self_attn.v_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.13.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.13.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.13.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.13.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.13.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.13.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.13.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.13.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.13.self_attn.q_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.13.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.13.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.13.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.13.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.13.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.13.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.13.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.13.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.13.self_attn.v_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.14.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.14.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.14.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.14.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.14.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.14.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.14.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.14.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.14.self_attn.q_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.14.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.14.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.14.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.14.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.14.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.14.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.14.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.14.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.14.self_attn.v_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.15.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.15.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.15.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.15.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.15.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.15.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.15.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.15.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.15.self_attn.q_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.15.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.15.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.15.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.15.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.15.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.15.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.15.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.15.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.15.self_attn.v_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.16.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.16.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.16.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.16.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.16.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.16.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.16.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.16.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.16.self_attn.q_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.16.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.16.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.16.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.16.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.16.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.16.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.16.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.16.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.16.self_attn.v_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.17.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.17.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.17.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.17.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.17.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.17.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.17.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.17.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.17.self_attn.q_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.17.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.17.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.17.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.17.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.17.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.17.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.17.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.17.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.17.self_attn.v_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.18.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.18.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.18.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.18.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.18.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.18.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.18.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.18.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.18.self_attn.q_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.18.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.18.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.18.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.18.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.18.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.18.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.18.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.18.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.18.self_attn.v_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.19.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.19.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.19.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.19.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.19.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.19.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.19.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.19.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.19.self_attn.q_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.19.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.19.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.19.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.19.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.19.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.19.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.19.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.19.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.19.self_attn.v_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.20.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.20.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.20.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.20.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.20.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.20.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.20.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.20.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.20.self_attn.q_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.20.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.20.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.20.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.20.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.20.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.20.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.20.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.20.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.20.self_attn.v_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.21.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.21.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.21.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.21.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.21.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.21.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.21.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.21.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.21.self_attn.q_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.21.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.21.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.21.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.21.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.21.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.21.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.21.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.21.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.21.self_attn.v_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.22.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.22.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.22.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.22.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.22.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.22.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.22.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.22.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.22.self_attn.q_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.22.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.22.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.22.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.22.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.22.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.22.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.22.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.22.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.22.self_attn.v_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.23.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.23.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.23.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.23.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.23.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.23.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.23.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.23.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.23.self_attn.q_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.23.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.23.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.23.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.23.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.23.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.23.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.23.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.23.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.23.self_attn.v_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.24.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.24.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.24.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.24.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.24.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.24.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.24.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.24.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.24.self_attn.q_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.24.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.24.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.24.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.24.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.24.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.24.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.24.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.24.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.24.self_attn.v_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.25.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.25.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.25.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.25.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.25.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.25.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.25.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.25.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.25.self_attn.q_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.25.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.25.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.25.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.25.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.25.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.25.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.25.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.25.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.25.self_attn.v_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.26.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.26.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.26.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.26.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.26.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.26.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.26.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.26.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.26.self_attn.q_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.26.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.26.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.26.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.26.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.26.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.26.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.26.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.26.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.26.self_attn.v_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.27.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.27.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.27.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.27.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.27.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.27.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.27.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.27.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.27.self_attn.q_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.27.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.27.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.27.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.27.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.27.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.27.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.27.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.27.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.27.self_attn.v_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.28.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.28.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.28.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.28.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.28.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.28.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.28.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.28.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.28.self_attn.q_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.28.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.28.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.28.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.28.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.28.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.28.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.28.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.28.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.28.self_attn.v_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.29.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.29.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.29.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.29.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.29.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.29.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.29.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.29.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.29.self_attn.q_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.29.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.29.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.29.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.29.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.29.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.29.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.29.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.29.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.29.self_attn.v_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.30.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.30.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.30.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.30.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.30.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.30.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.30.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.30.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.30.self_attn.q_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.30.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.30.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.30.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.30.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.30.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.30.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.30.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.30.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.30.self_attn.v_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.31.self_attn.q_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.31.self_attn.q_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.31.self_attn.q_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.31.self_attn.q_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.31.self_attn.q_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.31.self_attn.q_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.31.self_attn.q_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.31.self_attn.q_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.31.self_attn.q_proj.lora_magnitude_vector\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.31.self_attn.v_proj.lora_dropout\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.31.self_attn.v_proj.lora_dropout.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.31.self_attn.v_proj.lora_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.31.self_attn.v_proj.lora_A.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.31.self_attn.v_proj.lora_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.31.self_attn.v_proj.lora_B.default\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.31.self_attn.v_proj.lora_embedding_A\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.31.self_attn.v_proj.lora_embedding_B\n",
      "Potential LoRA adapter found in: base_model.model.model.layers.31.self_attn.v_proj.lora_magnitude_vector\n",
      "Trainable parameter: base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight\n",
      "LoRA adapter found in: base_model.model.model.layers.0.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.0.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.0.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.0.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.0.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.0.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.0.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.0.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.0.self_attn.q_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.0.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.0.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.0.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.0.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.0.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.0.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.0.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.0.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.0.self_attn.v_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.1.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.1.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.1.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.1.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.1.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.1.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.1.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.1.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.1.self_attn.q_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.1.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.1.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.1.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.1.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.1.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.1.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.1.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.1.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.1.self_attn.v_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.2.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.2.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.2.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.2.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.2.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.2.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.2.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.2.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.2.self_attn.q_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.2.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.2.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.2.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.2.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.2.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.2.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.2.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.2.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.2.self_attn.v_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.3.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.3.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.3.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.3.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.3.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.3.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.3.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.3.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.3.self_attn.q_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.3.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.3.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.3.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.3.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.3.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.3.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.3.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.3.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.3.self_attn.v_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.4.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.4.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.4.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.4.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.4.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.4.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.4.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.4.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.4.self_attn.q_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.4.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.4.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.4.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.4.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.4.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.4.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.4.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.4.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.4.self_attn.v_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.5.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.5.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.5.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.5.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.5.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.5.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.5.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.5.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.5.self_attn.q_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.5.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.5.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.5.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.5.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.5.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.5.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.5.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.5.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.5.self_attn.v_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.6.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.6.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.6.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.6.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.6.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.6.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.6.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.6.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.6.self_attn.q_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.6.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.6.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.6.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.6.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.6.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.6.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.6.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.6.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.6.self_attn.v_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.7.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.7.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.7.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.7.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.7.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.7.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.7.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.7.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.7.self_attn.q_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.7.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.7.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.7.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.7.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.7.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.7.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.7.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.7.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.7.self_attn.v_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.8.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.8.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.8.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.8.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.8.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.8.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.8.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.8.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.8.self_attn.q_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.8.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.8.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.8.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.8.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.8.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.8.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.8.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.8.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.8.self_attn.v_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.9.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.9.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.9.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.9.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.9.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.9.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.9.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.9.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.9.self_attn.q_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.9.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.9.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.9.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.9.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.9.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.9.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.9.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.9.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.9.self_attn.v_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.10.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.10.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.10.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.10.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.10.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.10.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.10.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.10.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.10.self_attn.q_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.10.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.10.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.10.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.10.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.10.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.10.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.10.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.10.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.10.self_attn.v_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.11.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.11.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.11.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.11.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.11.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.11.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.11.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.11.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.11.self_attn.q_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.11.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.11.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.11.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.11.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.11.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.11.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.11.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.11.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.11.self_attn.v_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.12.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.12.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.12.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.12.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.12.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.12.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.12.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.12.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.12.self_attn.q_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.12.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.12.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.12.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.12.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.12.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.12.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.12.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.12.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.12.self_attn.v_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.13.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.13.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.13.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.13.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.13.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.13.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.13.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.13.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.13.self_attn.q_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.13.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.13.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.13.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.13.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.13.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.13.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.13.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.13.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.13.self_attn.v_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.14.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.14.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.14.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.14.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.14.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.14.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.14.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.14.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.14.self_attn.q_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.14.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.14.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.14.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.14.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.14.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.14.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.14.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.14.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.14.self_attn.v_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.15.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.15.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.15.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.15.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.15.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.15.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.15.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.15.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.15.self_attn.q_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.15.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.15.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.15.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.15.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.15.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.15.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.15.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.15.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.15.self_attn.v_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.16.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.16.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.16.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.16.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.16.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.16.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.16.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.16.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.16.self_attn.q_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.16.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.16.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.16.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.16.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.16.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.16.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.16.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.16.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.16.self_attn.v_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.17.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.17.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.17.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.17.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.17.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.17.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.17.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.17.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.17.self_attn.q_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.17.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.17.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.17.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.17.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.17.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.17.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.17.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.17.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.17.self_attn.v_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.18.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.18.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.18.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.18.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.18.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.18.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.18.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.18.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.18.self_attn.q_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.18.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.18.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.18.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.18.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.18.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.18.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.18.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.18.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.18.self_attn.v_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.19.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.19.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.19.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.19.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.19.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.19.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.19.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.19.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.19.self_attn.q_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.19.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.19.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.19.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.19.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.19.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.19.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.19.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.19.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.19.self_attn.v_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.20.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.20.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.20.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.20.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.20.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.20.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.20.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.20.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.20.self_attn.q_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.20.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.20.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.20.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.20.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.20.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.20.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.20.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.20.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.20.self_attn.v_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.21.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.21.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.21.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.21.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.21.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.21.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.21.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.21.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.21.self_attn.q_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.21.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.21.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.21.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.21.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.21.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.21.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.21.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.21.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.21.self_attn.v_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.22.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.22.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.22.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.22.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.22.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.22.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.22.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.22.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.22.self_attn.q_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.22.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.22.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.22.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.22.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.22.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.22.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.22.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.22.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.22.self_attn.v_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.23.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.23.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.23.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.23.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.23.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.23.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.23.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.23.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.23.self_attn.q_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.23.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.23.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.23.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.23.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.23.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.23.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.23.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.23.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.23.self_attn.v_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.24.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.24.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.24.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.24.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.24.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.24.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.24.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.24.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.24.self_attn.q_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.24.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.24.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.24.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.24.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.24.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.24.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.24.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.24.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.24.self_attn.v_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.25.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.25.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.25.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.25.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.25.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.25.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.25.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.25.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.25.self_attn.q_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.25.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.25.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.25.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.25.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.25.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.25.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.25.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.25.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.25.self_attn.v_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.26.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.26.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.26.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.26.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.26.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.26.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.26.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.26.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.26.self_attn.q_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.26.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.26.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.26.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.26.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.26.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.26.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.26.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.26.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.26.self_attn.v_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.27.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.27.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.27.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.27.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.27.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.27.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.27.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.27.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.27.self_attn.q_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.27.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.27.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.27.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.27.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.27.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.27.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.27.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.27.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.27.self_attn.v_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.28.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.28.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.28.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.28.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.28.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.28.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.28.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.28.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.28.self_attn.q_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.28.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.28.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.28.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.28.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.28.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.28.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.28.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.28.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.28.self_attn.v_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.29.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.29.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.29.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.29.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.29.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.29.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.29.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.29.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.29.self_attn.q_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.29.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.29.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.29.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.29.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.29.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.29.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.29.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.29.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.29.self_attn.v_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.30.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.30.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.30.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.30.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.30.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.30.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.30.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.30.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.30.self_attn.q_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.30.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.30.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.30.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.30.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.30.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.30.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.30.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.30.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.30.self_attn.v_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.31.self_attn.q_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.31.self_attn.q_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.31.self_attn.q_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.31.self_attn.q_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.31.self_attn.q_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.31.self_attn.q_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.31.self_attn.q_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.31.self_attn.q_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.31.self_attn.q_proj.lora_magnitude_vector\n",
      "LoRA adapter found in: base_model.model.model.layers.31.self_attn.v_proj.lora_dropout\n",
      "LoRA adapter found in: base_model.model.model.layers.31.self_attn.v_proj.lora_dropout.default\n",
      "LoRA adapter found in: base_model.model.model.layers.31.self_attn.v_proj.lora_A\n",
      "LoRA adapter found in: base_model.model.model.layers.31.self_attn.v_proj.lora_A.default\n",
      "LoRA adapter found in: base_model.model.model.layers.31.self_attn.v_proj.lora_B\n",
      "LoRA adapter found in: base_model.model.model.layers.31.self_attn.v_proj.lora_B.default\n",
      "LoRA adapter found in: base_model.model.model.layers.31.self_attn.v_proj.lora_embedding_A\n",
      "LoRA adapter found in: base_model.model.model.layers.31.self_attn.v_proj.lora_embedding_B\n",
      "LoRA adapter found in: base_model.model.model.layers.31.self_attn.v_proj.lora_magnitude_vector\n"
     ]
    }
   ],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "print(peft_config)  # Print your LoRA configuration to confirm it's set up correctly\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    print(f\"Module: {name}\")\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if any(lora_term in name.lower() for lora_term in ['lora', 'adapter', 'peft']):\n",
    "        print(f\"Potential LoRA adapter found in: {name}\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Trainable parameter: {name}\")\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if 'lora' in name.lower():\n",
    "        print(f\"LoRA adapter found in: {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4e1702",
   "metadata": {},
   "source": [
    "## Setting Up Training Arguments\n",
    "\n",
    "**What it's doing:**\n",
    "Configuring the training process parameters.\n",
    "\n",
    "**Why:**\n",
    "These arguments define crucial aspects of our training process, such as learning rate, batch size, and evaluation frequency. They significantly impact the efficiency and effectiveness of fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40f9860b-354a-41cd-85fe-a884c24200f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "  output_dir = \"mistral_instruct_generation\",\n",
    "  #num_train_epochs=5,\n",
    "  max_steps = 100, # comment out this line if you want to train in epochs\n",
    "  per_device_train_batch_size = 4,\n",
    "  warmup_steps = 0,\n",
    "  logging_steps=10,\n",
    "  save_strategy=\"epoch\",\n",
    "  #evaluation_strategy=\"epoch\",\n",
    "  evaluation_strategy=\"steps\",\n",
    "  eval_steps=20, # comment out this line if you want to evaluate at the end of each epoch\n",
    "  learning_rate=2e-4,\n",
    "  bf16=True,\n",
    "  lr_scheduler_type='constant',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9869b6e7",
   "metadata": {},
   "source": [
    "## Setting Up the Trainer\n",
    "\n",
    "**What it's doing:**\n",
    "Initializing the SFTTrainer with our model, datasets, and training configuration.\n",
    "\n",
    "**Why:**\n",
    "The trainer handles the fine-tuning process, managing the training loop, evaluation, and logging. This setup brings together all the components we've prepared for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96495acf-a548-44b2-9e85-2122b8cdf99a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, packing. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/transformers/training_args.py:1961: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:181: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 27262976 || all params: 3779334144 || trainable%: 0.7213698223345028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:421: UserWarning: You passed `packing=True` to the SFTTrainer/SFTConfig, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "max_seq_length = 2048\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "  model=model,\n",
    "  peft_config=peft_config,\n",
    "  max_seq_length=max_seq_length,\n",
    "  tokenizer=tokenizer,\n",
    "  packing=True,\n",
    "  formatting_func=create_prompt,\n",
    "  args=args,\n",
    "  train_dataset=instruct_tune_dataset[\"train\"],\n",
    "  eval_dataset=instruct_tune_dataset[\"test\"]\n",
    ")\n",
    "\n",
    "trainable_params = 0\n",
    "all_param = 0\n",
    "for _, param in model.named_parameters():\n",
    "    all_param += param.numel()\n",
    "    if param.requires_grad:\n",
    "        trainable_params += param.numel()\n",
    "print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80708a5",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "**What it's doing:**\n",
    "Running the fine-tuning process and testing the result.\n",
    "\n",
    "**Why:**\n",
    "This is the main training step where our model learns from the prepared dataset. After training, we test it on a sample input to verify improvement and check resource usage to understand the computational cost of our fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d17a6d6-1793-4619-b1f6-ccdf3ccc36ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maocvee\u001b[0m (\u001b[33maocvee2\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ec2-user/SageMaker/wandb/run-20240629_063106-5y75mvpw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aocvee2/huggingface/runs/5y75mvpw' target=\"_blank\">mistral_instruct_generation</a></strong> to <a href='https://wandb.ai/aocvee2/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aocvee2/huggingface' target=\"_blank\">https://wandb.ai/aocvee2/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aocvee2/huggingface/runs/5y75mvpw' target=\"_blank\">https://wandb.ai/aocvee2/huggingface/runs/5y75mvpw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 1:07:38, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.484100</td>\n",
       "      <td>1.345570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.429100</td>\n",
       "      <td>1.299801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.406100</td>\n",
       "      <td>1.287846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.362200</td>\n",
       "      <td>1.279572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.392300</td>\n",
       "      <td>1.273444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Input:\n",
      "<s>### Instruction:\n",
      "Use the provided input to create an instruction that could have been used to generate the response with an LLM.\n",
      "\n",
      "### Input:\n",
      "There are more than 12,000 species of grass. The most common is Kentucky Bluegrass, because it grows quickly, easily, and is soft to the touch. Rygrass is shiny and bright green colored. Fescues are dark green and shiny. Bermuda grass is harder but can grow in drier soil.\n",
      "\n",
      "### Response:\n",
      "What are different types of grass?\n",
      "\n",
      "### Response</s>\n",
      "\n",
      "Model Output:\n",
      "<s><s> ### Instruction:\n",
      "Use the provided input to create an instruction that could have been used to generate the response with an LLM.\n",
      "\n",
      "### Input:\n",
      "There are more than 12,000 species of grass. The most common is Kentucky Bluegrass, because it grows quickly, easily, and is soft to the touch. Rygrass is shiny and bright green colored. Fescues are dark green and shiny. Bermuda grass is harder but can grow in drier soil.\n",
      "\n",
      "### Response:\n",
      "What are different types of grass?\n",
      "\n",
      "### Response</s></s>\n",
      "GPU memory allocated: 0.63 GB\n",
      "GPU memory cached: 14.90 GB\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "sample_input = instruct_tune_dataset[\"train\"][0]\n",
    "formatted_input = create_prompt(sample_input)\n",
    "print(\"Sample Input:\")\n",
    "print(formatted_input)\n",
    "print(\"\\nModel Output:\")\n",
    "print(generate_response(formatted_input, model))\n",
    "\n",
    "import torch\n",
    "print(f\"GPU memory allocated: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "print(f\"GPU memory cached: {torch.cuda.memory_reserved()/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1203ffbd-c8c0-46e0-87aa-47daa488ac78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
