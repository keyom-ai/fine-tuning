{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af2ee7fb-e888-4e38-a349-c7c40dfd2963",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "# Fine-tune LLaMA 2 models on SageMaker JumpStart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251624f9-1eb6-4051-a774-0a4ba83cabf5",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "---\n",
    "In this demo notebook, we demonstrate how to use the SageMaker Python SDK to deploy pre-trained Llama 2 model as well as fine-tune it for your dataset in domain adaptation or instruction tuning format.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d9b99d-639b-40f3-91e3-1fe00ee032a4",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "### Model License information\n",
    "---\n",
    "To perform inference on these models, you need to pass custom_attributes='accept_eula=true' as part of header. This means you have read and accept the end-user-license-agreement (EULA) of the model. EULA can be found in model card description or from https://ai.meta.com/resources/models-and-libraries/llama-downloads/. By default, this notebook sets custom_attributes='accept_eula=false', so all inference requests will fail until you explicitly change this custom attribute.\n",
    "\n",
    "Note: Custom_attributes used to pass EULA are key/value pairs. The key and value are separated by '=' and pairs are separated by ';'. If the user passes the same key more than once, the last value is kept and passed to the script handler (i.e., in this case, used for conditional logic). For example, if 'accept_eula=false; accept_eula=true' is passed to the server, then 'accept_eula=true' is kept and passed to the script handler.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019c4fcd-d6c5-4381-8425-1d224c0ac197",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "### Set up\n",
    "\n",
    "---\n",
    "We begin by installing and upgrading necessary packages. Restart the kernel after executing the cell below for the first time.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85addd9d-ec89-44a7-9fb5-9bc24fe9993b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (2.224.2)\n",
      "Requirement already satisfied: datasets in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (2.20.0)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (23.2.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.33.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (1.34.101)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (1.22.4)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (4.25.3)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (6.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (21.3)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (2.2.1)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (0.3.2)\n",
      "Requirement already satisfied: schema in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (0.7.7)\n",
      "Requirement already satisfied: PyYAML~=6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (6.0.1)\n",
      "Requirement already satisfied: jsonschema in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (4.21.1)\n",
      "Requirement already satisfied: platformdirs in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (4.2.0)\n",
      "Requirement already satisfied: tblib<4,>=1.7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (3.0.0)\n",
      "Requirement already satisfied: urllib3<3.0.0,>=1.26.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (2.2.1)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (2.32.3)\n",
      "Requirement already satisfied: docker in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (6.1.3)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (4.66.4)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (5.9.8)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets) (3.13.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets) (15.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets) (0.23.4)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.101 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker) (1.34.101)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker) (0.10.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.10.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker) (3.17.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from packaging>=20.0->sagemaker) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->sagemaker) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->sagemaker) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->sagemaker) (2024.2.2)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from docker->sagemaker) (1.7.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from google-pasta->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema->sagemaker) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.34.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.18.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->sagemaker) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->sagemaker) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->sagemaker) (2024.1)\n",
      "Requirement already satisfied: ppft>=1.7.6.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker) (1.7.6.8)\n",
      "Requirement already satisfied: pox>=0.3.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade sagemaker datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13274b9b-87bd-4090-a6aa-294570c31e0e",
   "metadata": {},
   "source": [
    "## Deploy Pre-trained Model\n",
    "\n",
    "---\n",
    "\n",
    "First we will deploy the Llama-2 model as a SageMaker endpoint. To train/deploy 13B and 70B models, please change model_id to \"meta-textgeneration-llama-2-7b\" and \"meta-textgeneration-llama-2-70b\" respectively.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7e01401-82db-4d49-9457-f930f4138618",
   "metadata": {
    "jumpStartAlterations": [
     "modelIdVersion"
    ],
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id, model_version = \"meta-textgeneration-llama-2-7b\", \"2.1.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1722b230-b7bc-487f-b4ee-98ca42848423",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using vulnerable JumpStart model 'meta-textgeneration-llama-2-7b' and version '2.1.3'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "pretrained_model = JumpStartModel(model_id=model_id, model_version=model_version)\n",
    "pretrained_predictor = pretrained_model.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8017c4ef-eb89-4da6-8e28-c800adbfc4b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Invoke the endpoint\n",
    "\n",
    "---\n",
    "Next, we invoke the endpoint with some sample queries. Later, in this notebook, we will fine-tune this model with a custom dataset and carry out inference using the fine-tuned model. We will also show comparison between results obtained via the pre-trained and the fine-tuned models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b795a085-048f-42b2-945f-0cd339c1cf91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_response(payload, response):\n",
    "    print(payload[\"inputs\"])\n",
    "    print(f\"> {response[0]['generation']}\")\n",
    "    print(\"\\n==================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dd833f8-1ddc-4805-80b2-19e7db629880",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I believe the meaning of life is\n",
      ">  to be happy.\n",
      "“I think we’re all born with the need to express ourselves, to make our mark on the world. Some people write poetry, some paint, some compose music, and some just live their lives and try to be the best they can be. I’m one of those people.\n",
      "\n",
      "==================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "payload = {\n",
    "    \"inputs\": \"I believe the meaning of life is\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 64,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.6,\n",
    "        \"return_full_text\": False,\n",
    "    },\n",
    "}\n",
    "try:\n",
    "    response = pretrained_predictor.predict(payload, custom_attributes=\"accept_eula=True\")\n",
    "    print_response(payload, response)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6773e6-7cf2-4cea-bce6-905d5995d857",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "To learn about additional use cases of pre-trained model, please checkout the notebook [Text completion: Run Llama 2 models in SageMaker JumpStart](https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/jumpstart-foundation-models/llama-2-text-completion.ipynb).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e19e16f-d459-40c6-9d6b-0272938b3878",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset preparation for fine-tuning\n",
    "\n",
    "---\n",
    "\n",
    "You can fine-tune on the dataset with domain adaptation format or instruction tuning format. Please find more details in the section [Dataset instruction](#Dataset-instruction). In this demo, we will use a subset of [Dolly dataset](https://huggingface.co/datasets/databricks/databricks-dolly-15k) in an instruction tuning format. Dolly dataset contains roughly 15,000 instruction following records for various categories such as question answering, summarization, information extraction etc. It is available under Apache 2.0 license. We will select the summarization examples for fine-tuning.\n",
    "\n",
    "\n",
    "Training data is formatted in JSON lines (.jsonl) format, where each line is a dictionary representing a single data sample. All training data must be in a single folder, however it can be saved in multiple jsonl files. The training folder can also contain a template.json file describing the input and output formats.\n",
    "\n",
    "To train your model on a collection of unstructured dataset (text files), please see the section [Example fine-tuning with Domain-Adaptation dataset format](#Example-fine-tuning-with-Domain-Adaptation-dataset-format) in the Appendix.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dd20a0d-15a5-49b0-a330-a75755d046ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a92e3d9769c241898283e2d2069f7250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2084958"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dolly_dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "\n",
    "# To train for question answering/information extraction, you can replace the assertion in next line to example[\"category\"] == \"closed_qa\"/\"information_extraction\".\n",
    "summarization_dataset = dolly_dataset.filter(lambda example: example[\"category\"] == \"summarization\")\n",
    "summarization_dataset = summarization_dataset.remove_columns(\"category\")\n",
    "\n",
    "# We split the dataset into two where test data is used to evaluate at the end.\n",
    "train_and_test_dataset = summarization_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "# Dumping the training data to a local file to be used for training.\n",
    "train_and_test_dataset[\"train\"].to_json(\"train.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9fbf002-3ee3-4cc8-8fce-871939f1bd19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Based on the reference text, please provide a short bulleted list of 4 popular Halloween activities.',\n",
       " 'context': \"Halloween or Hallowe'en (less commonly known as Allhalloween, All Hallows' Eve, or All Saints' Eve) is a celebration observed in many countries on 31 October, the eve of the Western Christian feast of All Saints' Day. It begins the observance of Allhallowtide, the time in the liturgical year dedicated to remembering the dead, including saints (hallows), martyrs, and all the faithful departed.\\n\\nOne theory holds that many Halloween traditions were influenced by Celtic harvest festivals, particularly the Gaelic festival Samhain, which are believed to have pagan roots. Some go further and suggest that Samhain may have been Christianized as All Hallow's Day, along with its eve, by the early Church. Other academics believe Halloween began solely as a Christian holiday, being the vigil of All Hallow's Day. Celebrated in Ireland and Scotland for centuries, Irish and Scottish immigrants took many Halloween customs to North America in the 19th century, and then through American influence Halloween had spread to other countries by the late 20th and early 21st century.\\n\\nPopular Halloween activities include trick-or-treating (or the related guising and souling), attending Halloween costume parties, carving pumpkins or turnips into jack-o'-lanterns, lighting bonfires, apple bobbing, divination games, playing pranks, visiting haunted attractions, telling scary stories, and watching horror or Halloween-themed films. Some people practice the Christian religious observances of All Hallows' Eve, including attending church services and lighting candles on the graves of the dead, although it is a secular celebration for others. Some Christians historically abstained from meat on All Hallows' Eve, a tradition reflected in the eating of certain vegetarian foods on this vigil day, including apples, potato pancakes, and soul cakes.\",\n",
       " 'response': 'Popular Halloween activities include:\\n1. Trick-or-Treating\\n2. Going to costume parties\\n3. Carving pumpkins\\n4. Watching horror movies'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_and_test_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2e5489-33dc-4623-92da-f6fc97bd25ab",
   "metadata": {},
   "source": [
    "---\n",
    "Next, we create a prompt template for using the data in an instruction / input format for the training job (since we are instruction fine-tuning the model in this example), and also for inferencing the deployed endpoint.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90451114-7cf5-445c-88e3-02ccaa5d3a4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "template = {\n",
    "    \"prompt\": \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "    \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{context}\\n\\n\",\n",
    "    \"completion\": \" {response}\",\n",
    "}\n",
    "with open(\"template.json\", \"w\") as f:\n",
    "    json.dump(template, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22171b1-1cec-4cec-9ce4-db62761633d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Upload dataset to S3\n",
    "---\n",
    "\n",
    "We will upload the prepared dataset to S3 which will be used for fine-tuning.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e1ee29a-8439-4788-8088-35a433fe2110",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: s3://sagemaker-us-east-1-577523854969/dolly_dataset\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "import sagemaker\n",
    "import random\n",
    "\n",
    "output_bucket = sagemaker.Session().default_bucket()\n",
    "local_data_file = \"train.jsonl\"\n",
    "train_data_location = f\"s3://{output_bucket}/dolly_dataset\"\n",
    "S3Uploader.upload(local_data_file, train_data_location)\n",
    "S3Uploader.upload(\"template.json\", train_data_location)\n",
    "print(f\"Training data: {train_data_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e61340-bc81-477d-aaf1-f37e8c554863",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train the model\n",
    "---\n",
    "Next, we fine-tune the LLaMA v2 7B model on the summarization dataset from Dolly. Finetuning scripts are based on scripts provided by [this repo](https://github.com/facebookresearch/llama-recipes/tree/main). To learn more about the fine-tuning scripts, please checkout section [5. Few notes about the fine-tuning method](#5.-Few-notes-about-the-fine-tuning-method). For a list of supported hyper-parameters and their default values, please see section [3. Supported Hyper-parameters for fine-tuning](#3.-Supported-Hyper-parameters-for-fine-tuning).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a71087e-9c9e-42d7-999e-5f3fac07bc4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: meta-textgeneration-llama-2-7b-2024-06-29-07-32-26-390\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-29 07:32:26 Starting - Starting the training job......\n",
      "2024-06-29 07:33:07 Starting - Preparing the instances for training...\n",
      "2024-06-29 07:33:29 Downloading - Downloading input data...........................\n",
      "2024-06-29 07:38:00 Downloading - Downloading the training image...\n",
      "2024-06-29 07:38:31 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-06-29 07:38:48,480 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-06-29 07:38:48,517 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-06-29 07:38:48,527 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-06-29 07:38:48,529 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-06-29 07:38:56,869 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing ./lib/accelerate/accelerate-0.21.0-py3-none-any.whl (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/bitsandbytes/bitsandbytes-0.39.1-py3-none-any.whl (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/black/black-23.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/brotli/Brotli-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/datasets/datasets-2.14.1-py3-none-any.whl (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/fire/fire-0.5.0.tar.gz\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mProcessing ./lib/inflate64/inflate64-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/loralib/loralib-0.1.1-py3-none-any.whl (from -r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/multivolumefile/multivolumefile-0.2.3-py3-none-any.whl (from -r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/mypy-extensions/mypy_extensions-1.0.0-py3-none-any.whl (from -r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pathspec/pathspec-0.11.1-py3-none-any.whl (from -r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/peft/peft-0.4.0-py3-none-any.whl (from -r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/py7zr/py7zr-0.20.5-py3-none-any.whl (from -r requirements.txt (line 13))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pybcj/pybcj-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 14))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pycryptodomex/pycryptodomex-3.18.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pyppmd/pyppmd-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 16))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pytorch-triton/pytorch_triton-2.1.0+e6216047b8-cp310-cp310-linux_x86_64.whl (from -r requirements.txt (line 17))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pyzstd/pyzstd-0.15.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 18))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/safetensors/safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 19))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/scipy/scipy-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 20))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/termcolor/termcolor-2.3.0-py3-none-any.whl (from -r requirements.txt (line 21))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/texttable/texttable-1.6.7-py2.py3-none-any.whl (from -r requirements.txt (line 22))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tokenize-rt/tokenize_rt-5.1.0-py2.py3-none-any.whl (from -r requirements.txt (line 23))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tokenizers/tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 24))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/torch/torch-2.1.0.dev20230905+cu118-cp310-cp310-linux_x86_64.whl (from -r requirements.txt (line 25))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/transformers/transformers-4.31.0-py3-none-any.whl (from -r requirements.txt (line 26))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_script_utilities/sagemaker_jumpstart_script_utilities-1.1.8-py2.py3-none-any.whl (from -r requirements.txt (line 27))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_huggingface_script_utilities/sagemaker_jumpstart_huggingface_script_utilities-1.0.7-py2.py3-none-any.whl (from -r requirements.txt (line 28))\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (1.24.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (8.1.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: platformdirs>=2 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (3.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tomli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (14.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2.31.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.11.1->datasets==2.14.1->-r requirements.txt (line 5)) (2023.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (3.9.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.20.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire==0.5.0->-r requirements.txt (line 6)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from pytorch-triton==2.1.0+e6216047b8->-r requirements.txt (line 17)) (3.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0.dev20230905+cu118->-r requirements.txt (line 25)) (4.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0.dev20230905+cu118->-r requirements.txt (line 25)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0.dev20230905+cu118->-r requirements.txt (line 25)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0.dev20230905+cu118->-r requirements.txt (line 25)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 26)) (2023.12.25)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (23.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (6.0.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (4.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (2024.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.1.0.dev20230905+cu118->-r requirements.txt (line 25)) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.1.0.dev20230905+cu118->-r requirements.txt (line 25)) (1.3.0)\u001b[0m\n",
      "\u001b[34mscipy is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\u001b[0m\n",
      "\u001b[34mtokenizers is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: fire\u001b[0m\n",
      "\u001b[34mBuilding wheel for fire (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for fire (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116932 sha256=6e99352cf936eb1c9d70454836f490300c9e8c08d0f2663dd74c020497b5eee8\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/db/3d/41/7e69dca5f61e37d109a4457082ffc5c6edb55ab633bafded38\u001b[0m\n",
      "\u001b[34mSuccessfully built fire\u001b[0m\n",
      "\u001b[34mInstalling collected packages: texttable, safetensors, Brotli, bitsandbytes, tokenize-rt, termcolor, sagemaker-jumpstart-script-utilities, sagemaker-jumpstart-huggingface-script-utilities, pyzstd, pytorch-triton, pyppmd, pycryptodomex, pybcj, pathspec, mypy-extensions, multivolumefile, loralib, inflate64, torch, py7zr, fire, black, transformers, accelerate, peft, datasets\u001b[0m\n",
      "\u001b[34mAttempting uninstall: torch\u001b[0m\n",
      "\u001b[34mFound existing installation: torch 2.0.0\u001b[0m\n",
      "\u001b[34mUninstalling torch-2.0.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled torch-2.0.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.28.1\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.28.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.28.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: datasets\u001b[0m\n",
      "\u001b[34mFound existing installation: datasets 2.16.1\u001b[0m\n",
      "\u001b[34mUninstalling datasets-2.16.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled datasets-2.16.1\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34mfastai 2.7.12 requires torch<2.1,>=1.7, but you have torch 2.1.0.dev20230905+cu118 which is incompatible.\u001b[0m\n",
      "\u001b[34mSuccessfully installed Brotli-1.0.9 accelerate-0.21.0 bitsandbytes-0.39.1 black-23.7.0 datasets-2.14.1 fire-0.5.0 inflate64-0.3.1 loralib-0.1.1 multivolumefile-0.2.3 mypy-extensions-1.0.0 pathspec-0.11.1 peft-0.4.0 py7zr-0.20.5 pybcj-1.0.1 pycryptodomex-3.18.0 pyppmd-1.0.0 pytorch-triton-2.1.0+e6216047b8 pyzstd-0.15.9 safetensors-0.3.1 sagemaker-jumpstart-huggingface-script-utilities-1.0.7 sagemaker-jumpstart-script-utilities-1.1.8 termcolor-2.3.0 texttable-1.6.7 tokenize-rt-5.1.0 torch-2.1.0.dev20230905+cu118 transformers-4.31.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2024-06-29 07:39:51,147 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-06-29 07:39:51,147 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-06-29 07:39:51,206 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-06-29 07:39:51,253 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-06-29 07:39:51,300 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-06-29 07:39:51,310 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"code\": \"/opt/ml/input/data/code\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"enable_fsdp\": \"True\",\n",
      "        \"epoch\": \"5\",\n",
      "        \"instruction_tuned\": \"True\",\n",
      "        \"int8_quantization\": \"False\",\n",
      "        \"learning_rate\": \"0.0001\",\n",
      "        \"lora_alpha\": \"32\",\n",
      "        \"lora_dropout\": \"0.05\",\n",
      "        \"lora_r\": \"8\",\n",
      "        \"max_input_length\": \"1024\",\n",
      "        \"max_train_samples\": \"-1\",\n",
      "        \"max_val_samples\": \"-1\",\n",
      "        \"per_device_eval_batch_size\": \"1\",\n",
      "        \"per_device_train_batch_size\": \"4\",\n",
      "        \"preprocessing_num_workers\": \"None\",\n",
      "        \"seed\": \"10\",\n",
      "        \"train_data_split_seed\": \"0\",\n",
      "        \"validation_split_ratio\": \"0.2\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"code\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.12xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"meta-textgeneration-llama-2-7b-2024-06-29-07-32-26-390\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"/opt/ml/input/data/code/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"transfer_learning\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 48,\n",
      "    \"num_gpus\": 4,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.12xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"transfer_learning.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"enable_fsdp\":\"True\",\"epoch\":\"5\",\"instruction_tuned\":\"True\",\"int8_quantization\":\"False\",\"learning_rate\":\"0.0001\",\"lora_alpha\":\"32\",\"lora_dropout\":\"0.05\",\"lora_r\":\"8\",\"max_input_length\":\"1024\",\"max_train_samples\":\"-1\",\"max_val_samples\":\"-1\",\"per_device_eval_batch_size\":\"1\",\"per_device_train_batch_size\":\"4\",\"preprocessing_num_workers\":\"None\",\"seed\":\"10\",\"train_data_split_seed\":\"0\",\"validation_split_ratio\":\"0.2\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=transfer_learning.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"code\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.12xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=transfer_learning\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=48\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=/opt/ml/input/data/code/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"code\":\"/opt/ml/input/data/code\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.12xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"enable_fsdp\":\"True\",\"epoch\":\"5\",\"instruction_tuned\":\"True\",\"int8_quantization\":\"False\",\"learning_rate\":\"0.0001\",\"lora_alpha\":\"32\",\"lora_dropout\":\"0.05\",\"lora_r\":\"8\",\"max_input_length\":\"1024\",\"max_train_samples\":\"-1\",\"max_val_samples\":\"-1\",\"per_device_eval_batch_size\":\"1\",\"per_device_train_batch_size\":\"4\",\"preprocessing_num_workers\":\"None\",\"seed\":\"10\",\"train_data_split_seed\":\"0\",\"validation_split_ratio\":\"0.2\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"meta-textgeneration-llama-2-7b-2024-06-29-07-32-26-390\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/input/data/code/sourcedir.tar.gz\",\"module_name\":\"transfer_learning\",\"network_interface_name\":\"eth0\",\"num_cpus\":48,\"num_gpus\":4,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"transfer_learning.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--enable_fsdp\",\"True\",\"--epoch\",\"5\",\"--instruction_tuned\",\"True\",\"--int8_quantization\",\"False\",\"--learning_rate\",\"0.0001\",\"--lora_alpha\",\"32\",\"--lora_dropout\",\"0.05\",\"--lora_r\",\"8\",\"--max_input_length\",\"1024\",\"--max_train_samples\",\"-1\",\"--max_val_samples\",\"-1\",\"--per_device_eval_batch_size\",\"1\",\"--per_device_train_batch_size\",\"4\",\"--preprocessing_num_workers\",\"None\",\"--seed\",\"10\",\"--train_data_split_seed\",\"0\",\"--validation_split_ratio\",\"0.2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_CODE=/opt/ml/input/data/code\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_ENABLE_FSDP=True\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCH=5\u001b[0m\n",
      "\u001b[34mSM_HP_INSTRUCTION_TUNED=True\u001b[0m\n",
      "\u001b[34mSM_HP_INT8_QUANTIZATION=False\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0001\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_ALPHA=32\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_DROPOUT=0.05\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_R=8\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_INPUT_LENGTH=1024\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_TRAIN_SAMPLES=-1\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_VAL_SAMPLES=-1\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=1\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[34mSM_HP_PREPROCESSING_NUM_WORKERS=None\u001b[0m\n",
      "\u001b[34mSM_HP_SEED=10\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_DATA_SPLIT_SEED=0\u001b[0m\n",
      "\u001b[34mSM_HP_VALIDATION_SPLIT_RATIO=0.2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 transfer_learning.py --enable_fsdp True --epoch 5 --instruction_tuned True --int8_quantization False --learning_rate 0.0001 --lora_alpha 32 --lora_dropout 0.05 --lora_r 8 --max_input_length 1024 --max_train_samples -1 --max_val_samples -1 --per_device_eval_batch_size 1 --per_device_train_batch_size 4 --preprocessing_num_workers None --seed 10 --train_data_split_seed 0 --validation_split_ratio 0.2\u001b[0m\n",
      "\u001b[34m2024-06-29 07:39:51,341 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34mINFO:root:Using pre-trained artifacts in SAGEMAKER_ADDITIONAL_S3_DATA_PATH=/opt/ml/additonals3data\u001b[0m\n",
      "\u001b[34mINFO:root:Identify file serving.properties in the un-tar directory /opt/ml/additonals3data. Copying it over to /opt/ml/model for model deployment after training is finished.\u001b[0m\n",
      "\u001b[34mINFO:root:Invoking the training command ['torchrun', '--nnodes', '1', '--nproc_per_node', '4', 'llama_finetuning.py', '--model_name', '/opt/ml/additonals3data', '--num_gpus', '4', '--pure_bf16', '--dist_checkpoint_root_folder', 'model_checkpoints', '--dist_checkpoint_folder', 'fine-tuned', '--batch_size_training', '4', '--micro_batch_size', '4', '--train_file', '/opt/ml/input/data/training', '--lr', '0.0001', '--do_train', '--output_dir', 'saved_peft_model', '--num_epochs', '5', '--use_peft', '--peft_method', 'lora', '--max_train_samples', '-1', '--max_val_samples', '-1', '--seed', '10', '--per_device_eval_batch_size', '1', '--max_input_length', '1024', '--preprocessing_num_workers', '--validation_split_ratio', '0.2', '--train_data_split_seed', '0', '--num_workers_dataloader', '0', '--weight_decay', '0.1', '--lora_r', '8', '--lora_alpha', '32', '--lora_dropout', '0.05', '--enable_fsdp', '--instruction_tuned'].\u001b[0m\n",
      "\u001b[34m[2024-06-29 07:39:57,228] torch.distributed.run: [WARNING] \u001b[0m\n",
      "\u001b[34m[2024-06-29 07:39:57,228] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34m[2024-06-29 07:39:57,228] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[34m[2024-06-29 07:39:57,228] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...CUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 0. Rank is 0\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 0\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34m--> Running with torch dist debug set to detail\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 2. Rank is 2\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 2\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 1. Rank is 1\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 1\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 3. Rank is 3\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 3\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mDownloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading data files: 100%|██████████| 1/1 [00:00<00:00, 13797.05it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files: 100%|██████████| 1/1 [00:00<00:00, 1819.65it/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 1069 examples [00:00, 53916.03 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:00<00:00, 16156.41 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:00<00:00, 15386.90 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:00<00:00, 15853.25 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 1000/1069 [00:02<00:00, 380.70 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 1000/1069 [00:02<00:00, 381.61 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 1000/1069 [00:02<00:00, 369.78 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 1000/1069 [00:02<00:00, 373.13 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:02<00:00, 384.13 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:02<00:00, 382.96 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:02<00:00, 385.44 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:02<00:00, 384.16 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:02<00:00, 373.98 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:02<00:00, 372.61 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:02<00:00, 377.28 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:02<00:00, 375.91 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 1000/1069 [00:00<00:00, 1647.87 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 1000/1069 [00:00<00:00, 1552.57 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:00<00:00, 1646.17 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Split the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 1000/1069 [00:00<00:00, 1646.16 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:00<00:00, 1553.91 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Split the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 1000/1069 [00:00<00:00, 1564.74 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:00<00:00, 1643.07 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Split the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:00<00:00, 1561.69 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Split the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.83s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:11<00:11, 11.83s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:11<00:11, 11.74s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:11<00:11, 11.88s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.26s/it]#015Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.94s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  6.22s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.05s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  6.26s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.09s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  6.28s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.12s/it]\u001b[0m\n",
      "\u001b[34m--> Model /opt/ml/additonals3data\u001b[0m\n",
      "\u001b[34m--> /opt/ml/additonals3data has 6738.415616 Million params\u001b[0m\n",
      "\u001b[34mtrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\u001b[0m\n",
      "\u001b[34mtrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\u001b[0m\n",
      "\u001b[34mbFloat16 enabled for mixed precision - using bfSixteen policy\u001b[0m\n",
      "\u001b[34mtrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\u001b[0m\n",
      "\u001b[34mtrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:329: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34mINFO:root:--> Training Set Length = 448\u001b[0m\n",
      "\u001b[34mINFO:root:--> Validation Set Length = 112\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:329: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:329: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mNCCL version 2.18.5+cuda11.8\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:329: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▎         #033[0m| 1/28 [00:12<05:28, 12.16s/it]\u001b[0m\n",
      "\u001b[34mstep 0 is completed and loss is 1.4508171081542969\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▎         #033[0m| 1/28 [00:12<05:30, 12.26s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▎         #033[0m| 1/28 [00:12<05:32, 12.33s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▎         #033[0m| 1/28 [00:12<05:42, 12.70s/it]\u001b[0m\n",
      "\u001b[34mstep 1 is completed and loss is 1.6329083442687988\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 2/28 [00:22<04:41, 10.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 2/28 [00:22<04:40, 10.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 2/28 [00:22<04:46, 11.03s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 2/28 [00:22<04:42, 10.87s/it]\u001b[0m\n",
      "\u001b[34mstep 2 is completed and loss is 1.4294370412826538\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 3/28 [00:31<04:19, 10.39s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 3/28 [00:32<04:22, 10.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 3/28 [00:32<04:20, 10.41s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 3/28 [00:31<04:19, 10.37s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 4/28 [00:42<04:05, 10.25s/it]\u001b[0m\n",
      "\u001b[34mstep 3 is completed and loss is 1.6073366403579712\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 4/28 [00:41<04:04, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 4/28 [00:41<04:04, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 4/28 [00:41<04:04, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 5/28 [00:52<03:52, 10.11s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 5/28 [00:51<03:51, 10.06s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 5/28 [00:51<03:51, 10.08s/it]\u001b[0m\n",
      "\u001b[34mstep 4 is completed and loss is 1.3113871812820435\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 5/28 [00:51<03:51, 10.07s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██▏       #033[0m| 6/28 [01:01<03:40, 10.00s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██▏       #033[0m| 6/28 [01:02<03:40, 10.03s/it]\u001b[0m\n",
      "\u001b[34mstep 5 is completed and loss is 1.4725590944290161\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██▏       #033[0m| 6/28 [01:01<03:40, 10.01s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██▏       #033[0m| 6/28 [01:01<03:40, 10.01s/it]\u001b[0m\n",
      "\u001b[34mstep 6 is completed and loss is 1.4898977279663086\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 7/28 [01:11<03:29,  9.95s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 7/28 [01:11<03:29,  9.96s/it]#015Training Epoch0:  25%|#033[34m██▌       #033[0m| 7/28 [01:11<03:29,  9.97s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 7/28 [01:11<03:29,  9.96s/it]\u001b[0m\n",
      "\u001b[34mstep 7 is completed and loss is 1.389475703239441\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▊       #033[0m| 8/28 [01:21<03:18,  9.93s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▊       #033[0m| 8/28 [01:21<03:18,  9.93s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▊       #033[0m| 8/28 [01:21<03:18,  9.92s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▊       #033[0m| 8/28 [01:21<03:18,  9.94s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 9/28 [01:31<03:08,  9.91s/it]\u001b[0m\n",
      "\u001b[34mstep 8 is completed and loss is 1.5408810377120972\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 9/28 [01:31<03:08,  9.91s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 9/28 [01:31<03:08,  9.91s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 9/28 [01:31<03:08,  9.92s/it]\u001b[0m\n",
      "\u001b[34mstep 9 is completed and loss is 1.577201247215271\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 10/28 [01:41<02:58,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 10/28 [01:41<02:58,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 10/28 [01:40<02:58,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 10/28 [01:41<02:58,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 11/28 [01:50<02:48,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 11/28 [01:51<02:48,  9.89s/it]#015Training Epoch0:  39%|#033[34m███▉      #033[0m| 11/28 [01:50<02:48,  9.88s/it]\u001b[0m\n",
      "\u001b[34mstep 10 is completed and loss is 1.6401753425598145\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 11/28 [01:50<02:48,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 12/28 [02:00<02:38,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 12/28 [02:00<02:38,  9.88s/it]\u001b[0m\n",
      "\u001b[34mstep 11 is completed and loss is 1.2909959554672241\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 12/28 [02:00<02:38,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 12/28 [02:01<02:38,  9.88s/it]\u001b[0m\n",
      "\u001b[34mstep 12 is completed and loss is 1.3219738006591797\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▋     #033[0m| 13/28 [02:10<02:28,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▋     #033[0m| 13/28 [02:10<02:28,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▋     #033[0m| 13/28 [02:11<02:28,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▋     #033[0m| 13/28 [02:10<02:28,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  50%|#033[34m█████     #033[0m| 14/28 [02:20<02:18,  9.88s/it]\u001b[0m\n",
      "\u001b[34mstep 13 is completed and loss is 1.1942431926727295\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  50%|#033[34m█████     #033[0m| 14/28 [02:20<02:18,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  50%|#033[34m█████     #033[0m| 14/28 [02:20<02:18,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  50%|#033[34m█████     #033[0m| 14/28 [02:20<02:18,  9.88s/it]\u001b[0m\n",
      "\u001b[34mstep 14 is completed and loss is 1.469440221786499\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  54%|#033[34m█████▎    #033[0m| 15/28 [02:30<02:08,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  54%|#033[34m█████▎    #033[0m| 15/28 [02:30<02:08,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  54%|#033[34m█████▎    #033[0m| 15/28 [02:30<02:08,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  54%|#033[34m█████▎    #033[0m| 15/28 [02:30<02:08,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 15 is completed and loss is 1.285390019416809\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  57%|#033[34m█████▋    #033[0m| 16/28 [02:40<01:58,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  57%|#033[34m█████▋    #033[0m| 16/28 [02:40<01:58,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  57%|#033[34m█████▋    #033[0m| 16/28 [02:40<01:58,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  57%|#033[34m█████▋    #033[0m| 16/28 [02:40<01:58,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  61%|#033[34m██████    #033[0m| 17/28 [02:49<01:48,  9.86s/it]#015Training Epoch0:  61%|#033[34m██████    #033[0m| 17/28 [02:50<01:48,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 16 is completed and loss is 1.5474950075149536\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  61%|#033[34m██████    #033[0m| 17/28 [02:50<01:48,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  61%|#033[34m██████    #033[0m| 17/28 [02:50<01:48,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  64%|#033[34m██████▍   #033[0m| 18/28 [02:59<01:38,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  64%|#033[34m██████▍   #033[0m| 18/28 [03:00<01:38,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 17 is completed and loss is 1.444138765335083\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  64%|#033[34m██████▍   #033[0m| 18/28 [02:59<01:38,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  64%|#033[34m██████▍   #033[0m| 18/28 [03:00<01:38,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 18 is completed and loss is 1.184810757637024\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  68%|#033[34m██████▊   #033[0m| 19/28 [03:09<01:28,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  68%|#033[34m██████▊   #033[0m| 19/28 [03:10<01:28,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  68%|#033[34m██████▊   #033[0m| 19/28 [03:09<01:28,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  68%|#033[34m██████▊   #033[0m| 19/28 [03:09<01:28,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  71%|#033[34m███████▏  #033[0m| 20/28 [03:19<01:18,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  71%|#033[34m███████▏  #033[0m| 20/28 [03:19<01:18,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 19 is completed and loss is 1.2024534940719604\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  71%|#033[34m███████▏  #033[0m| 20/28 [03:19<01:18,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  71%|#033[34m███████▏  #033[0m| 20/28 [03:20<01:18,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  75%|#033[34m███████▌  #033[0m| 21/28 [03:29<01:09,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  75%|#033[34m███████▌  #033[0m| 21/28 [03:29<01:09,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  75%|#033[34m███████▌  #033[0m| 21/28 [03:29<01:09,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 20 is completed and loss is 1.2835627794265747\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  75%|#033[34m███████▌  #033[0m| 21/28 [03:29<01:09,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 21 is completed and loss is 1.419262409210205\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  79%|#033[34m███████▊  #033[0m| 22/28 [03:39<00:59,  9.86s/it]#015Training Epoch0:  79%|#033[34m███████▊  #033[0m| 22/28 [03:39<00:59,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  79%|#033[34m███████▊  #033[0m| 22/28 [03:39<00:59,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  79%|#033[34m███████▊  #033[0m| 22/28 [03:39<00:59,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  82%|#033[34m████████▏ #033[0m| 23/28 [03:49<00:49,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  82%|#033[34m████████▏ #033[0m| 23/28 [03:49<00:49,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 22 is completed and loss is 1.2591158151626587\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  82%|#033[34m████████▏ #033[0m| 23/28 [03:49<00:49,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  82%|#033[34m████████▏ #033[0m| 23/28 [03:49<00:49,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  86%|#033[34m████████▌ #033[0m| 24/28 [03:58<00:39,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  86%|#033[34m████████▌ #033[0m| 24/28 [03:59<00:39,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  86%|#033[34m████████▌ #033[0m| 24/28 [03:59<00:39,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 23 is completed and loss is 1.315093755722046\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  86%|#033[34m████████▌ #033[0m| 24/28 [03:59<00:39,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 25/28 [04:09<00:29,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 25/28 [04:09<00:29,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 24 is completed and loss is 1.5467870235443115\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 25/28 [04:08<00:29,  9.86s/it]#015Training Epoch0:  89%|#033[34m████████▉ #033[0m| 25/28 [04:08<00:29,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 26/28 [04:18<00:19,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 25 is completed and loss is 1.5994750261306763\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 26/28 [04:18<00:19,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 26/28 [04:19<00:19,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 26/28 [04:18<00:19,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  96%|#033[34m█████████▋#033[0m| 27/28 [04:28<00:09,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  96%|#033[34m█████████▋#033[0m| 27/28 [04:28<00:09,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  96%|#033[34m█████████▋#033[0m| 27/28 [04:29<00:09,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 26 is completed and loss is 1.2814043760299683\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  96%|#033[34m█████████▋#033[0m| 27/28 [04:28<00:09,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 27 is completed and loss is 1.4597797393798828\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 28/28 [04:38<00:00,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 28/28 [04:38<00:00,  9.95s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 28/28 [04:39<00:00,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 28/28 [04:38<00:00,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 28/28 [04:39<00:00,  9.96s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 28/28 [04:38<00:00,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 28/28 [04:38<00:00,  9.95s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 28/28 [04:38<00:00,  9.95s/it]\u001b[0m\n",
      "\u001b[34mMax CUDA memory allocated was 8 GB\u001b[0m\n",
      "\u001b[34mMax CUDA memory reserved was 9 GB\u001b[0m\n",
      "\u001b[34mPeak active CUDA memory was 8 GB\u001b[0m\n",
      "\u001b[34mCuda Malloc retires : 0\u001b[0m\n",
      "\u001b[34mCPU Total Peak Memory consumed during the train (max): 1 GB\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:37,  3.62s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:37,  3.62s/it]#015evaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:37,  3.62s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:37,  3.62s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:32,  3.55s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:32,  3.55s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:32,  3.55s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:32,  3.55s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:28,  3.52s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:28,  3.52s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:28,  3.52s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:28,  3.52s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:14<01:24,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:14<01:24,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:14<01:24,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:14<01:24,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:20,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:20,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:20,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:20,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:21<01:16,  3.50s/it]#015evaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:21<01:16,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:21<01:16,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:21<01:16,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:13,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:13,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:13,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:13,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:28<01:09,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:28<01:09,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:28<01:09,  3.50s/it]#015evaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:28<01:09,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:06,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:06,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:06,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:06,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:35<01:02,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:35<01:02,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:35<01:02,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:35<01:02,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:59,  3.49s/it]#015evaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:59,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:59,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:59,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:42<00:55,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:42<00:55,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:42<00:55,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:42<00:55,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.49s/it]#015evaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:49<00:48,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:49<00:48,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:49<00:48,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:49<00:48,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.50s/it]#015evaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:56<00:41,  3.49s/it]#015evaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:56<00:41,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:56<00:41,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:56<00:41,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.49s/it]#015evaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.49s/it]#015evaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:06<00:31,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:06<00:31,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:06<00:31,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:06<00:31,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:13<00:24,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:13<00:24,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:13<00:24,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:13<00:24,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:20<00:17,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:20<00:17,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:20<00:17,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:20<00:17,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.49s/it]#015evaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:27<00:10,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:27<00:10,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:27<00:10,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:27<00:10,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:34<00:03,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:34<00:03,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:34<00:03,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:34<00:03,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.50s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.50s/it]\u001b[0m\n",
      "\u001b[34meval_ppl=tensor(3.6222, device='cuda:0') eval_epoch_loss=tensor(1.2871, device='cuda:0')\u001b[0m\n",
      "\u001b[34mwe are about to save the PEFT modules\u001b[0m\n",
      "\u001b[34mPEFT modules are saved in saved_peft_model directory\u001b[0m\n",
      "\u001b[34mbest eval loss on epoch 0 is 1.287090539932251\u001b[0m\n",
      "\u001b[34mEpoch 1: train_perplexity=4.1545, train_epoch_loss=1.4242, epcoh time 278.8354482369999s\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   4%|#033[34m▎         #033[0m| 1/28 [00:09<04:24,  9.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   4%|#033[34m▎         #033[0m| 1/28 [00:09<04:24,  9.80s/it]\u001b[0m\n",
      "\u001b[34mstep 0 is completed and loss is 1.2645093202590942\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   4%|#033[34m▎         #033[0m| 1/28 [00:09<04:24,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   4%|#033[34m▎         #033[0m| 1/28 [00:09<04:24,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   7%|#033[34m▋         #033[0m| 2/28 [00:19<04:15,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   7%|#033[34m▋         #033[0m| 2/28 [00:19<04:15,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   7%|#033[34m▋         #033[0m| 2/28 [00:19<04:15,  9.84s/it]\u001b[0m\n",
      "\u001b[34mstep 1 is completed and loss is 1.4276143312454224\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   7%|#033[34m▋         #033[0m| 2/28 [00:19<04:15,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  11%|#033[34m█         #033[0m| 3/28 [00:29<04:06,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  11%|#033[34m█         #033[0m| 3/28 [00:29<04:06,  9.84s/it]\u001b[0m\n",
      "\u001b[34mstep 2 is completed and loss is 1.1756194829940796\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  11%|#033[34m█         #033[0m| 3/28 [00:29<04:06,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  11%|#033[34m█         #033[0m| 3/28 [00:29<04:06,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 3 is completed and loss is 1.3563876152038574\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  14%|#033[34m█▍        #033[0m| 4/28 [00:39<03:56,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  14%|#033[34m█▍        #033[0m| 4/28 [00:39<03:56,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  14%|#033[34m█▍        #033[0m| 4/28 [00:39<03:56,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  14%|#033[34m█▍        #033[0m| 4/28 [00:39<03:56,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 4 is completed and loss is 1.0748685598373413\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  18%|#033[34m█▊        #033[0m| 5/28 [00:49<03:46,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  18%|#033[34m█▊        #033[0m| 5/28 [00:49<03:46,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  18%|#033[34m█▊        #033[0m| 5/28 [00:49<03:46,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  18%|#033[34m█▊        #033[0m| 5/28 [00:49<03:46,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  21%|#033[34m██▏       #033[0m| 6/28 [00:59<03:36,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  21%|#033[34m██▏       #033[0m| 6/28 [00:59<03:36,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  21%|#033[34m██▏       #033[0m| 6/28 [00:59<03:36,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 5 is completed and loss is 1.2467118501663208\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  21%|#033[34m██▏       #033[0m| 6/28 [00:59<03:36,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  25%|#033[34m██▌       #033[0m| 7/28 [01:08<03:27,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  25%|#033[34m██▌       #033[0m| 7/28 [01:08<03:27,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  25%|#033[34m██▌       #033[0m| 7/28 [01:08<03:27,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 6 is completed and loss is 1.3019113540649414\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  25%|#033[34m██▌       #033[0m| 7/28 [01:08<03:27,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 7 is completed and loss is 1.1929047107696533\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  29%|#033[34m██▊       #033[0m| 8/28 [01:18<03:17,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  29%|#033[34m██▊       #033[0m| 8/28 [01:18<03:17,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  29%|#033[34m██▊       #033[0m| 8/28 [01:18<03:17,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  29%|#033[34m██▊       #033[0m| 8/28 [01:18<03:17,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  32%|#033[34m███▏      #033[0m| 9/28 [01:28<03:07,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  32%|#033[34m███▏      #033[0m| 9/28 [01:28<03:07,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  32%|#033[34m███▏      #033[0m| 9/28 [01:28<03:07,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 8 is completed and loss is 1.3526753187179565\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  32%|#033[34m███▏      #033[0m| 9/28 [01:28<03:07,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  36%|#033[34m███▌      #033[0m| 10/28 [01:38<02:57,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 9 is completed and loss is 1.4210368394851685\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  36%|#033[34m███▌      #033[0m| 10/28 [01:38<02:57,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  36%|#033[34m███▌      #033[0m| 10/28 [01:38<02:57,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  36%|#033[34m███▌      #033[0m| 10/28 [01:38<02:57,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  39%|#033[34m███▉      #033[0m| 11/28 [01:48<02:47,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 10 is completed and loss is 1.4835906028747559\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  39%|#033[34m███▉      #033[0m| 11/28 [01:48<02:47,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  39%|#033[34m███▉      #033[0m| 11/28 [01:48<02:47,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  39%|#033[34m███▉      #033[0m| 11/28 [01:48<02:47,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  43%|#033[34m████▎     #033[0m| 12/28 [01:58<02:38,  9.89s/it]#015Training Epoch1:  43%|#033[34m████▎     #033[0m| 12/28 [01:58<02:38,  9.89s/it]\u001b[0m\n",
      "\u001b[34mstep 11 is completed and loss is 1.1440467834472656\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  43%|#033[34m████▎     #033[0m| 12/28 [01:58<02:38,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  43%|#033[34m████▎     #033[0m| 12/28 [01:58<02:38,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  46%|#033[34m████▋     #033[0m| 13/28 [02:08<02:28,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  46%|#033[34m████▋     #033[0m| 13/28 [02:08<02:28,  9.88s/it]#015Training Epoch1:  46%|#033[34m████▋     #033[0m| 13/28 [02:08<02:28,  9.88s/it]\u001b[0m\n",
      "\u001b[34mstep 12 is completed and loss is 1.1979185342788696\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  46%|#033[34m████▋     #033[0m| 13/28 [02:08<02:28,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  50%|#033[34m█████     #033[0m| 14/28 [02:18<02:18,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 13 is completed and loss is 1.0866814851760864\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  50%|#033[34m█████     #033[0m| 14/28 [02:18<02:18,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  50%|#033[34m█████     #033[0m| 14/28 [02:18<02:18,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  50%|#033[34m█████     #033[0m| 14/28 [02:18<02:18,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  54%|#033[34m█████▎    #033[0m| 15/28 [02:27<02:08,  9.87s/it]#015Training Epoch1:  54%|#033[34m█████▎    #033[0m| 15/28 [02:27<02:08,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  54%|#033[34m█████▎    #033[0m| 15/28 [02:27<02:08,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 14 is completed and loss is 1.3595598936080933\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  54%|#033[34m█████▎    #033[0m| 15/28 [02:27<02:08,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  57%|#033[34m█████▋    #033[0m| 16/28 [02:37<01:58,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  57%|#033[34m█████▋    #033[0m| 16/28 [02:37<01:58,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  57%|#033[34m█████▋    #033[0m| 16/28 [02:37<01:58,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 15 is completed and loss is 1.1789735555648804\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  57%|#033[34m█████▋    #033[0m| 16/28 [02:37<01:58,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  61%|#033[34m██████    #033[0m| 17/28 [02:47<01:48,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 16 is completed and loss is 1.487136721611023\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  61%|#033[34m██████    #033[0m| 17/28 [02:47<01:48,  9.87s/it]#015Training Epoch1:  61%|#033[34m██████    #033[0m| 17/28 [02:47<01:48,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  61%|#033[34m██████    #033[0m| 17/28 [02:47<01:48,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  64%|#033[34m██████▍   #033[0m| 18/28 [02:57<01:38,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 17 is completed and loss is 1.39582097530365\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  64%|#033[34m██████▍   #033[0m| 18/28 [02:57<01:38,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  64%|#033[34m██████▍   #033[0m| 18/28 [02:57<01:38,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  64%|#033[34m██████▍   #033[0m| 18/28 [02:57<01:38,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  68%|#033[34m██████▊   #033[0m| 19/28 [03:07<01:28,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 18 is completed and loss is 1.0975579023361206\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  68%|#033[34m██████▊   #033[0m| 19/28 [03:07<01:28,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  68%|#033[34m██████▊   #033[0m| 19/28 [03:07<01:28,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  68%|#033[34m██████▊   #033[0m| 19/28 [03:07<01:28,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 19 is completed and loss is 1.128583312034607\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  71%|#033[34m███████▏  #033[0m| 20/28 [03:17<01:18,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  71%|#033[34m███████▏  #033[0m| 20/28 [03:17<01:18,  9.86s/it]#015Training Epoch1:  71%|#033[34m███████▏  #033[0m| 20/28 [03:17<01:18,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  71%|#033[34m███████▏  #033[0m| 20/28 [03:17<01:18,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  75%|#033[34m███████▌  #033[0m| 21/28 [03:27<01:09,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  75%|#033[34m███████▌  #033[0m| 21/28 [03:27<01:09,  9.87s/it]#015Training Epoch1:  75%|#033[34m███████▌  #033[0m| 21/28 [03:27<01:09,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 20 is completed and loss is 1.2008094787597656\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  75%|#033[34m███████▌  #033[0m| 21/28 [03:27<01:09,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 21 is completed and loss is 1.3529456853866577\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  79%|#033[34m███████▊  #033[0m| 22/28 [03:36<00:59,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  79%|#033[34m███████▊  #033[0m| 22/28 [03:36<00:59,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  79%|#033[34m███████▊  #033[0m| 22/28 [03:36<00:59,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  79%|#033[34m███████▊  #033[0m| 22/28 [03:36<00:59,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  82%|#033[34m████████▏ #033[0m| 23/28 [03:46<00:49,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  82%|#033[34m████████▏ #033[0m| 23/28 [03:46<00:49,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  82%|#033[34m████████▏ #033[0m| 23/28 [03:46<00:49,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 22 is completed and loss is 1.1969767808914185\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  82%|#033[34m████████▏ #033[0m| 23/28 [03:46<00:49,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 23 is completed and loss is 1.2475244998931885\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  86%|#033[34m████████▌ #033[0m| 24/28 [03:56<00:39,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  86%|#033[34m████████▌ #033[0m| 24/28 [03:56<00:39,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  86%|#033[34m████████▌ #033[0m| 24/28 [03:56<00:39,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  86%|#033[34m████████▌ #033[0m| 24/28 [03:56<00:39,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  89%|#033[34m████████▉ #033[0m| 25/28 [04:06<00:29,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 24 is completed and loss is 1.5147970914840698\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  89%|#033[34m████████▉ #033[0m| 25/28 [04:06<00:29,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  89%|#033[34m████████▉ #033[0m| 25/28 [04:06<00:29,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  89%|#033[34m████████▉ #033[0m| 25/28 [04:06<00:29,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 25 is completed and loss is 1.5499975681304932\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  93%|#033[34m█████████▎#033[0m| 26/28 [04:16<00:19,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  93%|#033[34m█████████▎#033[0m| 26/28 [04:16<00:19,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  93%|#033[34m█████████▎#033[0m| 26/28 [04:16<00:19,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  93%|#033[34m█████████▎#033[0m| 26/28 [04:16<00:19,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  96%|#033[34m█████████▋#033[0m| 27/28 [04:26<00:09,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 26 is completed and loss is 1.2344975471496582\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  96%|#033[34m█████████▋#033[0m| 27/28 [04:26<00:09,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  96%|#033[34m█████████▋#033[0m| 27/28 [04:26<00:09,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  96%|#033[34m█████████▋#033[0m| 27/28 [04:26<00:09,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 28/28 [04:36<00:00,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 28/28 [04:36<00:00,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 27 is completed and loss is 1.419880747795105\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 28/28 [04:36<00:00,  9.87s/it]#015Training Epoch1: 100%|#033[34m██████████#033[0m| 28/28 [04:36<00:00,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 28/28 [04:36<00:00,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 28/28 [04:36<00:00,  9.86s/it]#015Training Epoch1: 100%|#033[34m██████████#033[0m| 28/28 [04:36<00:00,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 28/28 [04:36<00:00,  9.86s/it]\u001b[0m\n",
      "\u001b[34mMax CUDA memory allocated was 8 GB\u001b[0m\n",
      "\u001b[34mMax CUDA memory reserved was 9 GB\u001b[0m\n",
      "\u001b[34mPeak active CUDA memory was 8 GB\u001b[0m\n",
      "\u001b[34mCuda Malloc retires : 0\u001b[0m\n",
      "\u001b[34mCPU Total Peak Memory consumed during the train (max): 2 GB\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:37,  3.62s/it]#015evaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:37,  3.61s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:37,  3.62s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:37,  3.62s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:32,  3.54s/it]#015evaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:32,  3.54s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:32,  3.54s/it]#015evaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:32,  3.54s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.52s/it]#015evaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.52s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.52s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.52s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:14<01:24,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:14<01:24,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:14<01:24,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:14<01:24,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:20,  3.50s/it]#015evaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:20,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:20,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:20,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:21<01:16,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:21<01:16,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:21<01:16,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:21<01:16,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:13,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:13,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:13,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:13,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:28<01:09,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:28<01:09,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:28<01:09,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:28<01:09,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:06,  3.49s/it]#015evaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:06,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:06,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:06,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:35<01:02,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:35<01:02,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:35<01:02,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:35<01:02,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:59,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:59,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:59,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:59,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.49s/it]#015evaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.49s/it]#015evaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:06<00:31,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:06<00:31,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:06<00:31,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:06<00:31,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:13<00:24,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:13<00:24,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:13<00:24,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:13<00:24,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.49s/it]#015evaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:20<00:17,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:20<00:17,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:20<00:17,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:20<00:17,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.49s/it]#015evaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.49s/it]#015evaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:27<00:10,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:27<00:10,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:27<00:10,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:27<00:10,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:34<00:03,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:34<00:03,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:34<00:03,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:34<00:03,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.49s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.49s/it]\u001b[0m\n",
      "\u001b[34m#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.49s/it]\u001b[0m\n",
      "\u001b[34meval_ppl=tensor(3.4887, device='cuda:0') eval_epoch_loss=tensor(1.2495, device='cuda:0')\u001b[0m\n",
      "\u001b[34mwe are about to save the PEFT modules\u001b[0m\n",
      "\u001b[34mPEFT modules are saved in saved_peft_model directory\u001b[0m\n",
      "\u001b[34mbest eval loss on epoch 1 is 1.249529480934143\u001b[0m\n",
      "\u001b[34mEpoch 2: train_perplexity=3.6871, train_epoch_loss=1.3048, epcoh time 276.6928587430001s\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   4%|#033[34m▎         #033[0m| 1/28 [00:09<04:24,  9.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   4%|#033[34m▎         #033[0m| 1/28 [00:09<04:24,  9.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   4%|#033[34m▎         #033[0m| 1/28 [00:09<04:24,  9.80s/it]\u001b[0m\n",
      "\u001b[34mstep 0 is completed and loss is 1.2216308116912842\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   4%|#033[34m▎         #033[0m| 1/28 [00:09<04:24,  9.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   7%|#033[34m▋         #033[0m| 2/28 [00:19<04:15,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   7%|#033[34m▋         #033[0m| 2/28 [00:19<04:15,  9.84s/it]\u001b[0m\n",
      "\u001b[34mstep 1 is completed and loss is 1.3881639242172241\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   7%|#033[34m▋         #033[0m| 2/28 [00:19<04:15,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   7%|#033[34m▋         #033[0m| 2/28 [00:19<04:15,  9.84s/it]\u001b[0m\n",
      "\u001b[34mstep 2 is completed and loss is 1.1178510189056396\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  11%|#033[34m█         #033[0m| 3/28 [00:29<04:06,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  11%|#033[34m█         #033[0m| 3/28 [00:29<04:06,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  11%|#033[34m█         #033[0m| 3/28 [00:29<04:06,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  11%|#033[34m█         #033[0m| 3/28 [00:29<04:06,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 3 is completed and loss is 1.31281578540802\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  14%|#033[34m█▍        #033[0m| 4/28 [00:39<03:56,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  14%|#033[34m█▍        #033[0m| 4/28 [00:39<03:56,  9.86s/it]#015Training Epoch2:  14%|#033[34m█▍        #033[0m| 4/28 [00:39<03:56,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  14%|#033[34m█▍        #033[0m| 4/28 [00:39<03:56,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 4 is completed and loss is 1.0250674486160278\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  18%|#033[34m█▊        #033[0m| 5/28 [00:49<03:46,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  18%|#033[34m█▊        #033[0m| 5/28 [00:49<03:46,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  18%|#033[34m█▊        #033[0m| 5/28 [00:49<03:46,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  18%|#033[34m█▊        #033[0m| 5/28 [00:49<03:46,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  21%|#033[34m██▏       #033[0m| 6/28 [00:59<03:37,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  21%|#033[34m██▏       #033[0m| 6/28 [00:59<03:37,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  21%|#033[34m██▏       #033[0m| 6/28 [00:59<03:37,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 5 is completed and loss is 1.1982454061508179\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  21%|#033[34m██▏       #033[0m| 6/28 [00:59<03:37,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 6 is completed and loss is 1.262800693511963\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  25%|#033[34m██▌       #033[0m| 7/28 [01:09<03:27,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  25%|#033[34m██▌       #033[0m| 7/28 [01:09<03:27,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  25%|#033[34m██▌       #033[0m| 7/28 [01:09<03:27,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  25%|#033[34m██▌       #033[0m| 7/28 [01:09<03:27,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  29%|#033[34m██▊       #033[0m| 8/28 [01:18<03:17,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 7 is completed and loss is 1.1495909690856934\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  29%|#033[34m██▊       #033[0m| 8/28 [01:18<03:17,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  29%|#033[34m██▊       #033[0m| 8/28 [01:18<03:17,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  29%|#033[34m██▊       #033[0m| 8/28 [01:18<03:17,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  32%|#033[34m███▏      #033[0m| 9/28 [01:28<03:07,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  32%|#033[34m███▏      #033[0m| 9/28 [01:28<03:07,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 8 is completed and loss is 1.3107730150222778\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  32%|#033[34m███▏      #033[0m| 9/28 [01:28<03:07,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  32%|#033[34m███▏      #033[0m| 9/28 [01:28<03:07,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 9 is completed and loss is 1.3788180351257324\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  36%|#033[34m███▌      #033[0m| 10/28 [01:38<02:57,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  36%|#033[34m███▌      #033[0m| 10/28 [01:38<02:57,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  36%|#033[34m███▌      #033[0m| 10/28 [01:38<02:57,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  36%|#033[34m███▌      #033[0m| 10/28 [01:38<02:57,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  39%|#033[34m███▉      #033[0m| 11/28 [01:48<02:47,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 10 is completed and loss is 1.4477635622024536\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  39%|#033[34m███▉      #033[0m| 11/28 [01:48<02:47,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  39%|#033[34m███▉      #033[0m| 11/28 [01:48<02:47,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  39%|#033[34m███▉      #033[0m| 11/28 [01:48<02:47,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 11 is completed and loss is 1.1039040088653564\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  43%|#033[34m████▎     #033[0m| 12/28 [01:58<02:38,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  43%|#033[34m████▎     #033[0m| 12/28 [01:58<02:38,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  43%|#033[34m████▎     #033[0m| 12/28 [01:58<02:38,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  43%|#033[34m████▎     #033[0m| 12/28 [01:58<02:38,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  46%|#033[34m████▋     #033[0m| 13/28 [02:08<02:28,  9.89s/it]\u001b[0m\n",
      "\u001b[34mstep 12 is completed and loss is 1.1621894836425781\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  46%|#033[34m████▋     #033[0m| 13/28 [02:08<02:28,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  46%|#033[34m████▋     #033[0m| 13/28 [02:08<02:28,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  46%|#033[34m████▋     #033[0m| 13/28 [02:08<02:28,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  50%|#033[34m█████     #033[0m| 14/28 [02:18<02:18,  9.89s/it]\u001b[0m\n",
      "\u001b[34mstep 13 is completed and loss is 1.0504742860794067\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  50%|#033[34m█████     #033[0m| 14/28 [02:18<02:18,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  50%|#033[34m█████     #033[0m| 14/28 [02:18<02:18,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  50%|#033[34m█████     #033[0m| 14/28 [02:18<02:18,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  54%|#033[34m█████▎    #033[0m| 15/28 [02:28<02:08,  9.88s/it]#015Training Epoch2:  54%|#033[34m█████▎    #033[0m| 15/28 [02:28<02:08,  9.88s/it]\u001b[0m\n",
      "\u001b[34mstep 14 is completed and loss is 1.3207300901412964\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  54%|#033[34m█████▎    #033[0m| 15/28 [02:28<02:08,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  54%|#033[34m█████▎    #033[0m| 15/28 [02:28<02:08,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  57%|#033[34m█████▋    #033[0m| 16/28 [02:37<01:58,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  57%|#033[34m█████▋    #033[0m| 16/28 [02:37<01:58,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 15 is completed and loss is 1.1423949003219604\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  57%|#033[34m█████▋    #033[0m| 16/28 [02:37<01:58,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  57%|#033[34m█████▋    #033[0m| 16/28 [02:37<01:58,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  61%|#033[34m██████    #033[0m| 17/28 [02:47<01:48,  9.87s/it]#015Training Epoch2:  61%|#033[34m██████    #033[0m| 17/28 [02:47<01:48,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  61%|#033[34m██████    #033[0m| 17/28 [02:47<01:48,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 16 is completed and loss is 1.4612189531326294\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  61%|#033[34m██████    #033[0m| 17/28 [02:47<01:48,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  64%|#033[34m██████▍   #033[0m| 18/28 [02:57<01:38,  9.87s/it]#015Training Epoch2:  64%|#033[34m██████▍   #033[0m| 18/28 [02:57<01:38,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  64%|#033[34m██████▍   #033[0m| 18/28 [02:57<01:38,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 17 is completed and loss is 1.381277084350586\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  64%|#033[34m██████▍   #033[0m| 18/28 [02:57<01:38,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  68%|#033[34m██████▊   #033[0m| 19/28 [03:07<01:28,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  68%|#033[34m██████▊   #033[0m| 19/28 [03:07<01:28,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  68%|#033[34m██████▊   #033[0m| 19/28 [03:07<01:28,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 18 is completed and loss is 1.066562294960022\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  68%|#033[34m██████▊   #033[0m| 19/28 [03:07<01:28,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  71%|#033[34m███████▏  #033[0m| 20/28 [03:17<01:18,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  71%|#033[34m███████▏  #033[0m| 20/28 [03:17<01:18,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  71%|#033[34m███████▏  #033[0m| 20/28 [03:17<01:18,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 19 is completed and loss is 1.0975826978683472\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  71%|#033[34m███████▏  #033[0m| 20/28 [03:17<01:18,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  75%|#033[34m███████▌  #033[0m| 21/28 [03:27<01:09,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  75%|#033[34m███████▌  #033[0m| 21/28 [03:27<01:09,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 20 is completed and loss is 1.1707309484481812\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  75%|#033[34m███████▌  #033[0m| 21/28 [03:27<01:09,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  75%|#033[34m███████▌  #033[0m| 21/28 [03:27<01:09,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  79%|#033[34m███████▊  #033[0m| 22/28 [03:37<00:59,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 21 is completed and loss is 1.3264743089675903\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  79%|#033[34m███████▊  #033[0m| 22/28 [03:37<00:59,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  79%|#033[34m███████▊  #033[0m| 22/28 [03:37<00:59,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  79%|#033[34m███████▊  #033[0m| 22/28 [03:37<00:59,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  82%|#033[34m████████▏ #033[0m| 23/28 [03:47<00:49,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  82%|#033[34m████████▏ #033[0m| 23/28 [03:47<00:49,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  82%|#033[34m████████▏ #033[0m| 23/28 [03:47<00:49,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 22 is completed and loss is 1.1685919761657715\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  82%|#033[34m████████▏ #033[0m| 23/28 [03:47<00:49,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  86%|#033[34m████████▌ #033[0m| 24/28 [03:56<00:39,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  86%|#033[34m████████▌ #033[0m| 24/28 [03:56<00:39,  9.88s/it]\u001b[0m\n",
      "\u001b[34mstep 23 is completed and loss is 1.2160377502441406\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  86%|#033[34m████████▌ #033[0m| 24/28 [03:56<00:39,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  86%|#033[34m████████▌ #033[0m| 24/28 [03:56<00:39,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  89%|#033[34m████████▉ #033[0m| 25/28 [04:06<00:29,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  89%|#033[34m████████▉ #033[0m| 25/28 [04:06<00:29,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  89%|#033[34m████████▉ #033[0m| 25/28 [04:06<00:29,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 24 is completed and loss is 1.4975733757019043\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  89%|#033[34m████████▉ #033[0m| 25/28 [04:06<00:29,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  93%|#033[34m█████████▎#033[0m| 26/28 [04:16<00:19,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  93%|#033[34m█████████▎#033[0m| 26/28 [04:16<00:19,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  93%|#033[34m█████████▎#033[0m| 26/28 [04:16<00:19,  9.88s/it]\u001b[0m\n",
      "\u001b[34mstep 25 is completed and loss is 1.5312355756759644\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  93%|#033[34m█████████▎#033[0m| 26/28 [04:16<00:19,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  96%|#033[34m█████████▋#033[0m| 27/28 [04:26<00:09,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  96%|#033[34m█████████▋#033[0m| 27/28 [04:26<00:09,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  96%|#033[34m█████████▋#033[0m| 27/28 [04:26<00:09,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 26 is completed and loss is 1.2138323783874512\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  96%|#033[34m█████████▋#033[0m| 27/28 [04:26<00:09,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 27 is completed and loss is 1.401145100593567\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 28/28 [04:36<00:00,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 28/28 [04:36<00:00,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 28/28 [04:36<00:00,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 28/28 [04:36<00:00,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 28/28 [04:36<00:00,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 28/28 [04:36<00:00,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 28/28 [04:36<00:00,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 28/28 [04:36<00:00,  9.87s/it]\u001b[0m\n",
      "\u001b[34mMax CUDA memory allocated was 8 GB\u001b[0m\n",
      "\u001b[34mMax CUDA memory reserved was 9 GB\u001b[0m\n",
      "\u001b[34mPeak active CUDA memory was 8 GB\u001b[0m\n",
      "\u001b[34mCuda Malloc retires : 0\u001b[0m\n",
      "\u001b[34mCPU Total Peak Memory consumed during the train (max): 2 GB\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:37,  3.62s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:37,  3.62s/it]#015evaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:37,  3.61s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:37,  3.62s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:32,  3.55s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:32,  3.55s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:32,  3.54s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:32,  3.55s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:28,  3.52s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:28,  3.52s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:28,  3.52s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:28,  3.52s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:14<01:24,  3.51s/it]#015evaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:14<01:24,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:14<01:24,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:14<01:24,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:20,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:20,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:20,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:20,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:21<01:17,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:21<01:17,  3.50s/it]#015evaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:21<01:17,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:21<01:17,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:13,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:13,  3.50s/it]#015evaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:13,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:13,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:28<01:09,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:28<01:09,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:28<01:09,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:28<01:09,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:06,  3.49s/it]#015evaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:06,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:06,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:06,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:35<01:02,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:35<01:02,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:35<01:02,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:35<01:02,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:59,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:59,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:59,  3.49s/it]#015evaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:59,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:42<00:55,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:42<00:55,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:42<00:55,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:42<00:55,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.48s/it]#015evaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:06<00:31,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:06<00:31,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:06<00:31,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:06<00:31,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:13<00:24,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:13<00:24,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:13<00:24,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:13<00:24,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:20<00:17,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:20<00:17,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:20<00:17,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:20<00:17,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:27<00:10,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:27<00:10,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:27<00:10,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:27<00:10,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:34<00:03,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:34<00:03,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:34<00:03,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:34<00:03,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.49s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.49s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.49s/it]\u001b[0m\n",
      "\u001b[34meval_ppl=tensor(3.4465, device='cuda:0') eval_epoch_loss=tensor(1.2373, device='cuda:0')\u001b[0m\n",
      "\u001b[34mwe are about to save the PEFT modules\u001b[0m\n",
      "\u001b[34mPEFT modules are saved in saved_peft_model directory\u001b[0m\n",
      "\u001b[34mbest eval loss on epoch 2 is 1.2373497486114502\u001b[0m\n",
      "\u001b[34mEpoch 3: train_perplexity=3.5702, train_epoch_loss=1.2726, epcoh time 276.90017691599996s\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   4%|#033[34m▎         #033[0m| 1/28 [00:09<04:24,  9.80s/it]#015Training Epoch3:   4%|#033[34m▎         #033[0m| 1/28 [00:09<04:24,  9.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   4%|#033[34m▎         #033[0m| 1/28 [00:09<04:24,  9.80s/it]\u001b[0m\n",
      "\u001b[34mstep 0 is completed and loss is 1.2017244100570679\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   4%|#033[34m▎         #033[0m| 1/28 [00:09<04:24,  9.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   7%|#033[34m▋         #033[0m| 2/28 [00:19<04:15,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   7%|#033[34m▋         #033[0m| 2/28 [00:19<04:15,  9.84s/it]#015Training Epoch3:   7%|#033[34m▋         #033[0m| 2/28 [00:19<04:15,  9.84s/it]\u001b[0m\n",
      "\u001b[34mstep 1 is completed and loss is 1.368615984916687\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   7%|#033[34m▋         #033[0m| 2/28 [00:19<04:15,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  11%|#033[34m█         #033[0m| 3/28 [00:29<04:06,  9.85s/it]#015Training Epoch3:  11%|#033[34m█         #033[0m| 3/28 [00:29<04:06,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 2 is completed and loss is 1.0948803424835205\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  11%|#033[34m█         #033[0m| 3/28 [00:29<04:06,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  11%|#033[34m█         #033[0m| 3/28 [00:29<04:06,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  14%|#033[34m█▍        #033[0m| 4/28 [00:39<03:56,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  14%|#033[34m█▍        #033[0m| 4/28 [00:39<03:56,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 3 is completed and loss is 1.290258526802063\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  14%|#033[34m█▍        #033[0m| 4/28 [00:39<03:56,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  14%|#033[34m█▍        #033[0m| 4/28 [00:39<03:56,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  18%|#033[34m█▊        #033[0m| 5/28 [00:49<03:46,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  18%|#033[34m█▊        #033[0m| 5/28 [00:49<03:46,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  18%|#033[34m█▊        #033[0m| 5/28 [00:49<03:46,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 4 is completed and loss is 1.0038868188858032\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  18%|#033[34m█▊        #033[0m| 5/28 [00:49<03:46,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 5 is completed and loss is 1.1788052320480347\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  21%|#033[34m██▏       #033[0m| 6/28 [00:59<03:36,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  21%|#033[34m██▏       #033[0m| 6/28 [00:59<03:36,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  21%|#033[34m██▏       #033[0m| 6/28 [00:59<03:36,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  21%|#033[34m██▏       #033[0m| 6/28 [00:59<03:36,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  25%|#033[34m██▌       #033[0m| 7/28 [01:08<03:27,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 6 is completed and loss is 1.2433841228485107\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  25%|#033[34m██▌       #033[0m| 7/28 [01:08<03:27,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  25%|#033[34m██▌       #033[0m| 7/28 [01:08<03:27,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  25%|#033[34m██▌       #033[0m| 7/28 [01:08<03:27,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  29%|#033[34m██▊       #033[0m| 8/28 [01:18<03:17,  9.86s/it]#015Training Epoch3:  29%|#033[34m██▊       #033[0m| 8/28 [01:18<03:17,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 7 is completed and loss is 1.1324559450149536\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  29%|#033[34m██▊       #033[0m| 8/28 [01:18<03:17,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  29%|#033[34m██▊       #033[0m| 8/28 [01:18<03:17,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  32%|#033[34m███▏      #033[0m| 9/28 [01:28<03:07,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 8 is completed and loss is 1.2922627925872803\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  32%|#033[34m███▏      #033[0m| 9/28 [01:28<03:07,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  32%|#033[34m███▏      #033[0m| 9/28 [01:28<03:07,  9.86s/it]#015Training Epoch3:  32%|#033[34m███▏      #033[0m| 9/28 [01:28<03:07,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  36%|#033[34m███▌      #033[0m| 10/28 [01:38<02:57,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 9 is completed and loss is 1.3587570190429688\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  36%|#033[34m███▌      #033[0m| 10/28 [01:38<02:57,  9.86s/it]#015Training Epoch3:  36%|#033[34m███▌      #033[0m| 10/28 [01:38<02:57,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  36%|#033[34m███▌      #033[0m| 10/28 [01:38<02:57,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  39%|#033[34m███▉      #033[0m| 11/28 [01:48<02:47,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 10 is completed and loss is 1.4302752017974854\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  39%|#033[34m███▉      #033[0m| 11/28 [01:48<02:47,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  39%|#033[34m███▉      #033[0m| 11/28 [01:48<02:47,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  39%|#033[34m███▉      #033[0m| 11/28 [01:48<02:47,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  43%|#033[34m████▎     #033[0m| 12/28 [01:58<02:38,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  43%|#033[34m████▎     #033[0m| 12/28 [01:58<02:38,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  43%|#033[34m████▎     #033[0m| 12/28 [01:58<02:38,  9.90s/it]\u001b[0m\n",
      "\u001b[34mstep 11 is completed and loss is 1.0835546255111694\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  43%|#033[34m████▎     #033[0m| 12/28 [01:58<02:38,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  46%|#033[34m████▋     #033[0m| 13/28 [02:08<02:28,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  46%|#033[34m████▋     #033[0m| 13/28 [02:08<02:28,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  46%|#033[34m████▋     #033[0m| 13/28 [02:08<02:28,  9.89s/it]\u001b[0m\n",
      "\u001b[34mstep 12 is completed and loss is 1.1436930894851685\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  46%|#033[34m████▋     #033[0m| 13/28 [02:08<02:28,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  50%|#033[34m█████     #033[0m| 14/28 [02:18<02:18,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  50%|#033[34m█████     #033[0m| 14/28 [02:18<02:18,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  50%|#033[34m█████     #033[0m| 14/28 [02:18<02:18,  9.88s/it]\u001b[0m\n",
      "\u001b[34mstep 13 is completed and loss is 1.0348014831542969\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  50%|#033[34m█████     #033[0m| 14/28 [02:18<02:18,  9.88s/it]\u001b[0m\n",
      "\u001b[34mstep 14 is completed and loss is 1.3052620887756348\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  54%|#033[34m█████▎    #033[0m| 15/28 [02:27<02:08,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  54%|#033[34m█████▎    #033[0m| 15/28 [02:27<02:08,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  54%|#033[34m█████▎    #033[0m| 15/28 [02:27<02:08,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  54%|#033[34m█████▎    #033[0m| 15/28 [02:27<02:08,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  57%|#033[34m█████▋    #033[0m| 16/28 [02:37<01:58,  9.87s/it]#015Training Epoch3:  57%|#033[34m█████▋    #033[0m| 16/28 [02:37<01:58,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  57%|#033[34m█████▋    #033[0m| 16/28 [02:37<01:58,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 15 is completed and loss is 1.1269619464874268\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  57%|#033[34m█████▋    #033[0m| 16/28 [02:37<01:58,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  61%|#033[34m██████    #033[0m| 17/28 [02:47<01:48,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 16 is completed and loss is 1.4485182762145996\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  61%|#033[34m██████    #033[0m| 17/28 [02:47<01:48,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  61%|#033[34m██████    #033[0m| 17/28 [02:47<01:48,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  61%|#033[34m██████    #033[0m| 17/28 [02:47<01:48,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 17 is completed and loss is 1.3705239295959473\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  64%|#033[34m██████▍   #033[0m| 18/28 [02:57<01:38,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  64%|#033[34m██████▍   #033[0m| 18/28 [02:57<01:38,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  64%|#033[34m██████▍   #033[0m| 18/28 [02:57<01:38,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  64%|#033[34m██████▍   #033[0m| 18/28 [02:57<01:38,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  68%|#033[34m██████▊   #033[0m| 19/28 [03:07<01:28,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  68%|#033[34m██████▊   #033[0m| 19/28 [03:07<01:28,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  68%|#033[34m██████▊   #033[0m| 19/28 [03:07<01:28,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 18 is completed and loss is 1.0507360696792603\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  68%|#033[34m██████▊   #033[0m| 19/28 [03:07<01:28,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 19 is completed and loss is 1.0840290784835815\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  71%|#033[34m███████▏  #033[0m| 20/28 [03:17<01:18,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  71%|#033[34m███████▏  #033[0m| 20/28 [03:17<01:18,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  71%|#033[34m███████▏  #033[0m| 20/28 [03:17<01:18,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  71%|#033[34m███████▏  #033[0m| 20/28 [03:17<01:18,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  75%|#033[34m███████▌  #033[0m| 21/28 [03:27<01:09,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  75%|#033[34m███████▌  #033[0m| 21/28 [03:27<01:09,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  75%|#033[34m███████▌  #033[0m| 21/28 [03:27<01:09,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 20 is completed and loss is 1.157558560371399\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  75%|#033[34m███████▌  #033[0m| 21/28 [03:27<01:09,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  79%|#033[34m███████▊  #033[0m| 22/28 [03:36<00:59,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  79%|#033[34m███████▊  #033[0m| 22/28 [03:36<00:59,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  79%|#033[34m███████▊  #033[0m| 22/28 [03:36<00:59,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 21 is completed and loss is 1.310414433479309\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  79%|#033[34m███████▊  #033[0m| 22/28 [03:36<00:59,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 22 is completed and loss is 1.1512906551361084\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  82%|#033[34m████████▏ #033[0m| 23/28 [03:46<00:49,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  82%|#033[34m████████▏ #033[0m| 23/28 [03:46<00:49,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  82%|#033[34m████████▏ #033[0m| 23/28 [03:46<00:49,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  82%|#033[34m████████▏ #033[0m| 23/28 [03:46<00:49,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  86%|#033[34m████████▌ #033[0m| 24/28 [03:56<00:39,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  86%|#033[34m████████▌ #033[0m| 24/28 [03:56<00:39,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 23 is completed and loss is 1.2013235092163086\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  86%|#033[34m████████▌ #033[0m| 24/28 [03:56<00:39,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  86%|#033[34m████████▌ #033[0m| 24/28 [03:56<00:39,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 24 is completed and loss is 1.4808558225631714\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  89%|#033[34m████████▉ #033[0m| 25/28 [04:06<00:29,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  89%|#033[34m████████▉ #033[0m| 25/28 [04:06<00:29,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  89%|#033[34m████████▉ #033[0m| 25/28 [04:06<00:29,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  89%|#033[34m████████▉ #033[0m| 25/28 [04:06<00:29,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 25 is completed and loss is 1.5178523063659668\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  93%|#033[34m█████████▎#033[0m| 26/28 [04:16<00:19,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  93%|#033[34m█████████▎#033[0m| 26/28 [04:16<00:19,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  93%|#033[34m█████████▎#033[0m| 26/28 [04:16<00:19,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  93%|#033[34m█████████▎#033[0m| 26/28 [04:16<00:19,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  96%|#033[34m█████████▋#033[0m| 27/28 [04:26<00:09,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  96%|#033[34m█████████▋#033[0m| 27/28 [04:26<00:09,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  96%|#033[34m█████████▋#033[0m| 27/28 [04:26<00:09,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 26 is completed and loss is 1.2009248733520508\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  96%|#033[34m█████████▋#033[0m| 27/28 [04:26<00:09,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 28/28 [04:36<00:00,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 28/28 [04:36<00:00,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 28/28 [04:36<00:00,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 28/28 [04:36<00:00,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 27 is completed and loss is 1.3856475353240967\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 28/28 [04:36<00:00,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 28/28 [04:36<00:00,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 28/28 [04:36<00:00,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 28/28 [04:36<00:00,  9.86s/it]\u001b[0m\n",
      "\u001b[34mMax CUDA memory allocated was 8 GB\u001b[0m\n",
      "\u001b[34mMax CUDA memory reserved was 9 GB\u001b[0m\n",
      "\u001b[34mPeak active CUDA memory was 8 GB\u001b[0m\n",
      "\u001b[34mCuda Malloc retires : 0\u001b[0m\n",
      "\u001b[34mCPU Total Peak Memory consumed during the train (max): 2 GB\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:37,  3.63s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:37,  3.63s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:37,  3.62s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:37,  3.62s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:32,  3.54s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:32,  3.54s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:32,  3.54s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:32,  3.54s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.52s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.52s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.52s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.52s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:14<01:24,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:14<01:24,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:14<01:23,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:14<01:23,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:20,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:20,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:20,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:20,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:21<01:16,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:21<01:16,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:21<01:16,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:21<01:16,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:13,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:13,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:13,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:13,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:28<01:09,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:28<01:09,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:28<01:09,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:28<01:09,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:06,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:06,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:06,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:06,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:59,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:59,  3.49s/it]#015evaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:59,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:59,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.49s/it]#015evaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.49s/it]#015evaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.49s/it]#015evaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:06<00:31,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:06<00:31,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:06<00:31,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:06<00:31,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:13<00:24,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:13<00:24,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:13<00:24,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:13<00:24,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.49s/it]#015evaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:20<00:17,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:20<00:17,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:20<00:17,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:20<00:17,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:27<00:10,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:27<00:10,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:27<00:10,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:27<00:10,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:34<00:03,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:34<00:03,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:34<00:03,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:34<00:03,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.49s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.49s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.49s/it]\u001b[0m\n",
      "\u001b[34meval_ppl=tensor(3.4325, device='cuda:0') eval_epoch_loss=tensor(1.2333, device='cuda:0')\u001b[0m\n",
      "\u001b[34mwe are about to save the PEFT modules\u001b[0m\n",
      "\u001b[34mPEFT modules are saved in saved_peft_model directory\u001b[0m\n",
      "\u001b[34mbest eval loss on epoch 3 is 1.2332922220230103\u001b[0m\n",
      "\u001b[34mEpoch 4: train_perplexity=3.5105, train_epoch_loss=1.2557, epcoh time 276.6927552479999s\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   4%|#033[34m▎         #033[0m| 1/28 [00:09<04:24,  9.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   4%|#033[34m▎         #033[0m| 1/28 [00:09<04:24,  9.79s/it]\u001b[0m\n",
      "\u001b[34mstep 0 is completed and loss is 1.1856411695480347\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   4%|#033[34m▎         #033[0m| 1/28 [00:09<04:24,  9.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   4%|#033[34m▎         #033[0m| 1/28 [00:09<04:24,  9.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   7%|#033[34m▋         #033[0m| 2/28 [00:19<04:15,  9.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   7%|#033[34m▋         #033[0m| 2/28 [00:19<04:15,  9.83s/it]\u001b[0m\n",
      "\u001b[34mstep 1 is completed and loss is 1.3545411825180054\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   7%|#033[34m▋         #033[0m| 2/28 [00:19<04:15,  9.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   7%|#033[34m▋         #033[0m| 2/28 [00:19<04:15,  9.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  11%|#033[34m█         #033[0m| 3/28 [00:29<04:06,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 2 is completed and loss is 1.0816726684570312\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  11%|#033[34m█         #033[0m| 3/28 [00:29<04:06,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  11%|#033[34m█         #033[0m| 3/28 [00:29<04:06,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  11%|#033[34m█         #033[0m| 3/28 [00:29<04:06,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  14%|#033[34m█▍        #033[0m| 4/28 [00:39<03:56,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  14%|#033[34m█▍        #033[0m| 4/28 [00:39<03:56,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  14%|#033[34m█▍        #033[0m| 4/28 [00:39<03:56,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 3 is completed and loss is 1.2728040218353271\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  14%|#033[34m█▍        #033[0m| 4/28 [00:39<03:56,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  18%|#033[34m█▊        #033[0m| 5/28 [00:49<03:46,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 4 is completed and loss is 0.9881665110588074\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  18%|#033[34m█▊        #033[0m| 5/28 [00:49<03:46,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  18%|#033[34m█▊        #033[0m| 5/28 [00:49<03:46,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  18%|#033[34m█▊        #033[0m| 5/28 [00:49<03:46,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  21%|#033[34m██▏       #033[0m| 6/28 [00:59<03:36,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  21%|#033[34m██▏       #033[0m| 6/28 [00:59<03:36,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 5 is completed and loss is 1.1655690670013428\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  21%|#033[34m██▏       #033[0m| 6/28 [00:59<03:36,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  21%|#033[34m██▏       #033[0m| 6/28 [00:59<03:36,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  25%|#033[34m██▌       #033[0m| 7/28 [01:08<03:27,  9.86s/it]#015Training Epoch4:  25%|#033[34m██▌       #033[0m| 7/28 [01:08<03:27,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  25%|#033[34m██▌       #033[0m| 7/28 [01:08<03:27,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 6 is completed and loss is 1.227852463722229\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  25%|#033[34m██▌       #033[0m| 7/28 [01:08<03:27,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  29%|#033[34m██▊       #033[0m| 8/28 [01:18<03:17,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  29%|#033[34m██▊       #033[0m| 8/28 [01:18<03:17,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  29%|#033[34m██▊       #033[0m| 8/28 [01:18<03:17,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 7 is completed and loss is 1.1201516389846802\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  29%|#033[34m██▊       #033[0m| 8/28 [01:18<03:17,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  32%|#033[34m███▏      #033[0m| 9/28 [01:28<03:07,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  32%|#033[34m███▏      #033[0m| 9/28 [01:28<03:07,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  32%|#033[34m███▏      #033[0m| 9/28 [01:28<03:07,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 8 is completed and loss is 1.2796539068222046\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  32%|#033[34m███▏      #033[0m| 9/28 [01:28<03:07,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  36%|#033[34m███▌      #033[0m| 10/28 [01:38<02:57,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  36%|#033[34m███▌      #033[0m| 10/28 [01:38<02:57,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  36%|#033[34m███▌      #033[0m| 10/28 [01:38<02:57,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 9 is completed and loss is 1.3429114818572998\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  36%|#033[34m███▌      #033[0m| 10/28 [01:38<02:57,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  39%|#033[34m███▉      #033[0m| 11/28 [01:48<02:47,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  39%|#033[34m███▉      #033[0m| 11/28 [01:48<02:47,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  39%|#033[34m███▉      #033[0m| 11/28 [01:48<02:47,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 10 is completed and loss is 1.4169996976852417\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  39%|#033[34m███▉      #033[0m| 11/28 [01:48<02:47,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  43%|#033[34m████▎     #033[0m| 12/28 [01:58<02:38,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  43%|#033[34m████▎     #033[0m| 12/28 [01:58<02:38,  9.90s/it]\u001b[0m\n",
      "\u001b[34mstep 11 is completed and loss is 1.0672225952148438\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  43%|#033[34m████▎     #033[0m| 12/28 [01:58<02:38,  9.90s/it]#015Training Epoch4:  43%|#033[34m████▎     #033[0m| 12/28 [01:58<02:38,  9.90s/it]\u001b[0m\n",
      "\u001b[34mstep 12 is completed and loss is 1.1296740770339966\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  46%|#033[34m████▋     #033[0m| 13/28 [02:08<02:28,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  46%|#033[34m████▋     #033[0m| 13/28 [02:08<02:28,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  46%|#033[34m████▋     #033[0m| 13/28 [02:08<02:28,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  46%|#033[34m████▋     #033[0m| 13/28 [02:08<02:28,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  50%|#033[34m█████     #033[0m| 14/28 [02:18<02:18,  9.88s/it]\u001b[0m\n",
      "\u001b[34mstep 13 is completed and loss is 1.0231372117996216\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  50%|#033[34m█████     #033[0m| 14/28 [02:18<02:18,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  50%|#033[34m█████     #033[0m| 14/28 [02:18<02:18,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  50%|#033[34m█████     #033[0m| 14/28 [02:18<02:18,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  54%|#033[34m█████▎    #033[0m| 15/28 [02:28<02:08,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  54%|#033[34m█████▎    #033[0m| 15/28 [02:28<02:08,  9.88s/it]\u001b[0m\n",
      "\u001b[34mstep 14 is completed and loss is 1.292891025543213\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  54%|#033[34m█████▎    #033[0m| 15/28 [02:28<02:08,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  54%|#033[34m█████▎    #033[0m| 15/28 [02:28<02:08,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  57%|#033[34m█████▋    #033[0m| 16/28 [02:37<01:58,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 15 is completed and loss is 1.1130505800247192\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  57%|#033[34m█████▋    #033[0m| 16/28 [02:37<01:58,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  57%|#033[34m█████▋    #033[0m| 16/28 [02:37<01:58,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  57%|#033[34m█████▋    #033[0m| 16/28 [02:37<01:58,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  61%|#033[34m██████    #033[0m| 17/28 [02:47<01:48,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  61%|#033[34m██████    #033[0m| 17/28 [02:47<01:48,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 16 is completed and loss is 1.433326005935669\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  61%|#033[34m██████    #033[0m| 17/28 [02:47<01:48,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  61%|#033[34m██████    #033[0m| 17/28 [02:47<01:48,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  64%|#033[34m██████▍   #033[0m| 18/28 [02:57<01:38,  9.86s/it]#015Training Epoch4:  64%|#033[34m██████▍   #033[0m| 18/28 [02:57<01:38,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 17 is completed and loss is 1.3606576919555664\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  64%|#033[34m██████▍   #033[0m| 18/28 [02:57<01:38,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  64%|#033[34m██████▍   #033[0m| 18/28 [02:57<01:38,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  68%|#033[34m██████▊   #033[0m| 19/28 [03:07<01:28,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  68%|#033[34m██████▊   #033[0m| 19/28 [03:07<01:28,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 18 is completed and loss is 1.0341389179229736\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  68%|#033[34m██████▊   #033[0m| 19/28 [03:07<01:28,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  68%|#033[34m██████▊   #033[0m| 19/28 [03:07<01:28,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  71%|#033[34m███████▏  #033[0m| 20/28 [03:17<01:18,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  71%|#033[34m███████▏  #033[0m| 20/28 [03:17<01:18,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 19 is completed and loss is 1.0730332136154175\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  71%|#033[34m███████▏  #033[0m| 20/28 [03:17<01:18,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  71%|#033[34m███████▏  #033[0m| 20/28 [03:17<01:18,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  75%|#033[34m███████▌  #033[0m| 21/28 [03:27<01:09,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  75%|#033[34m███████▌  #033[0m| 21/28 [03:27<01:09,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  75%|#033[34m███████▌  #033[0m| 21/28 [03:27<01:09,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 20 is completed and loss is 1.1469330787658691\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  75%|#033[34m███████▌  #033[0m| 21/28 [03:27<01:09,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  79%|#033[34m███████▊  #033[0m| 22/28 [03:37<00:59,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  79%|#033[34m███████▊  #033[0m| 22/28 [03:37<00:59,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  79%|#033[34m███████▊  #033[0m| 22/28 [03:37<00:59,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 21 is completed and loss is 1.2933399677276611\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  79%|#033[34m███████▊  #033[0m| 22/28 [03:37<00:59,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  82%|#033[34m████████▏ #033[0m| 23/28 [03:46<00:49,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  82%|#033[34m████████▏ #033[0m| 23/28 [03:46<00:49,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 22 is completed and loss is 1.1373755931854248\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  82%|#033[34m████████▏ #033[0m| 23/28 [03:46<00:49,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  82%|#033[34m████████▏ #033[0m| 23/28 [03:46<00:49,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  86%|#033[34m████████▌ #033[0m| 24/28 [03:56<00:39,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  86%|#033[34m████████▌ #033[0m| 24/28 [03:56<00:39,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 23 is completed and loss is 1.1896222829818726\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  86%|#033[34m████████▌ #033[0m| 24/28 [03:56<00:39,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  86%|#033[34m████████▌ #033[0m| 24/28 [03:56<00:39,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  89%|#033[34m████████▉ #033[0m| 25/28 [04:06<00:29,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 24 is completed and loss is 1.466031789779663\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  89%|#033[34m████████▉ #033[0m| 25/28 [04:06<00:29,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  89%|#033[34m████████▉ #033[0m| 25/28 [04:06<00:29,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  89%|#033[34m████████▉ #033[0m| 25/28 [04:06<00:29,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  93%|#033[34m█████████▎#033[0m| 26/28 [04:16<00:19,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  93%|#033[34m█████████▎#033[0m| 26/28 [04:16<00:19,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 25 is completed and loss is 1.5048798322677612\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  93%|#033[34m█████████▎#033[0m| 26/28 [04:16<00:19,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  93%|#033[34m█████████▎#033[0m| 26/28 [04:16<00:19,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  96%|#033[34m█████████▋#033[0m| 27/28 [04:26<00:09,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 26 is completed and loss is 1.1884896755218506\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  96%|#033[34m█████████▋#033[0m| 27/28 [04:26<00:09,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  96%|#033[34m█████████▋#033[0m| 27/28 [04:26<00:09,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  96%|#033[34m█████████▋#033[0m| 27/28 [04:26<00:09,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 28/28 [04:36<00:00,  9.86s/it]#015Training Epoch4: 100%|#033[34m██████████#033[0m| 28/28 [04:36<00:00,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 28/28 [04:36<00:00,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 28/28 [04:36<00:00,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 27 is completed and loss is 1.3733649253845215\u001b[0m\n",
      "\u001b[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 28/28 [04:36<00:00,  9.86s/it]#015Training Epoch4: 100%|#033[34m██████████#033[0m| 28/28 [04:36<00:00,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 28/28 [04:36<00:00,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 28/28 [04:36<00:00,  9.86s/it]\u001b[0m\n",
      "\u001b[34mMax CUDA memory allocated was 8 GB\u001b[0m\n",
      "\u001b[34mMax CUDA memory reserved was 9 GB\u001b[0m\n",
      "\u001b[34mPeak active CUDA memory was 8 GB\u001b[0m\n",
      "\u001b[34mCuda Malloc retires : 0\u001b[0m\n",
      "\u001b[34mCPU Total Peak Memory consumed during the train (max): 2 GB\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:37,  3.61s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:37,  3.61s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:37,  3.61s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:37,  3.61s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:32,  3.54s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.54s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.54s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:32,  3.54s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.51s/it]#015evaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:14<01:24,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:14<01:24,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:14<01:24,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:14<01:24,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:20,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:20,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:20,  3.50s/it]#015evaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:20,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:21<01:16,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:21<01:16,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:21<01:16,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:21<01:16,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:13,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:13,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:13,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:13,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:28<01:09,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:28<01:09,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:28<01:09,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:28<01:09,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:06,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:06,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:06,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:06,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:35<01:02,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:35<01:02,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:35<01:02,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:35<01:02,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:59,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:59,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:59,  3.49s/it]#015evaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:59,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:42<00:55,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:42<00:55,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:42<00:55,  3.49s/it]#015evaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:42<00:55,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:49<00:48,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:49<00:48,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:49<00:48,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:49<00:48,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:56<00:41,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:56<00:41,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:56<00:41,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:56<00:41,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.50s/it]#015evaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.50s/it]#015evaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:03<00:34,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:03<00:34,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:03<00:34,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:03<00:34,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:06<00:31,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:06<00:31,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:06<00:31,  3.50s/it]#015evaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:06<00:31,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:13<00:24,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:13<00:24,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:13<00:24,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:13<00:24,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.49s/it]#015evaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:20<00:17,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:20<00:17,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:20<00:17,  3.49s/it]#015evaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:20<00:17,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:27<00:10,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:27<00:10,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:27<00:10,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:27<00:10,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:34<00:03,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:34<00:03,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:34<00:03,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:34<00:03,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.50s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.50s/it]\u001b[0m\n",
      "\u001b[34meval_ppl=tensor(3.4288, device='cuda:0') eval_epoch_loss=tensor(1.2322, device='cuda:0')\u001b[0m\n",
      "\u001b[34mwe are about to save the PEFT modules\u001b[0m\n",
      "\u001b[34mPEFT modules are saved in saved_peft_model directory\u001b[0m\n",
      "\u001b[34mbest eval loss on epoch 4 is 1.2321999073028564\u001b[0m\n",
      "\u001b[34mEpoch 5: train_perplexity=3.4640, train_epoch_loss=1.2424, epcoh time 276.72205407900015s\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_train_prep, Value: 3.6772403717041016\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_train_loss, Value: 1.2999622821807861\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_eval_prep, Value: 3.4837353229522705\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_eval_loss, Value: 1.2478923797607422\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_epoch_time, Value: 277.1686586446\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_checkpoint_time, Value: 3.508366716400019\u001b[0m\n",
      "\u001b[34mINFO:root:Combining pre-trained base model with the PEFT adapter module.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.18it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.80it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.67it/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Saving the combined model in safetensors format.\u001b[0m\n",
      "\u001b[34mINFO:root:Saving complete.\u001b[0m\n",
      "\u001b[34mINFO:root:Copying tokenizer to the output directory.\u001b[0m\n",
      "\u001b[34mINFO:root:Putting inference code with the fine-tuned model directory.\u001b[0m\n",
      "\u001b[34m2024-06-29 08:15:05,869 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-06-29 08:15:05,869 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-06-29 08:15:05,869 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-06-29 08:15:13 Uploading - Uploading generated training model\n",
      "2024-06-29 08:16:06 Completed - Training job completed\n",
      "Training seconds: 2556\n",
      "Billable seconds: 2556\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "\n",
    "# lora_r: Lora R. Must be a positive integer. Default: 8.\n",
    "# lora_alpha: Lora Alpha. Must be a positive integer. Default: 32\n",
    "# lora_dropout: Lora Dropout. must be a positive float between 0 and 1. Default: 0.05. \n",
    "\n",
    "\n",
    "estimator = JumpStartEstimator(\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    environment={\"accept_eula\": \"true\"},\n",
    "    tolerate_vulnerable_model=True,\n",
    "    disable_output_compression=True,  # For Llama-2-70b, add instance_type = \"ml.g5.48xlarge\"\n",
    ")\n",
    "# By default, instruction tuning is set to false. Thus, to use instruction tuning dataset you use\n",
    "estimator.set_hyperparameters(instruction_tuned=\"True\", epoch=\"5\", max_input_length=\"1024\")\n",
    "estimator.fit({\"training\": train_data_location})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9decbf-08c6-4cb4-8644-4a96afb5bebf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Deploy the fine-tuned model\n",
    "---\n",
    "Next, we deploy fine-tuned model. We will compare the performance of fine-tuned and pre-trained model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "016e591b-63f8-4e0f-941c-4b4e0b9dc6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No instance type selected for inference hosting endpoint. Defaulting to ml.g5.2xlarge.\n",
      "INFO:sagemaker.jumpstart:No instance type selected for inference hosting endpoint. Defaulting to ml.g5.2xlarge.\n",
      "INFO:sagemaker:Creating model with name: meta-textgeneration-llama-2-7b-2024-06-29-08-16-43-092\n",
      "INFO:sagemaker:Creating endpoint-config with name meta-textgeneration-llama-2-7b-2024-06-29-08-16-43-090\n",
      "INFO:sagemaker:Creating endpoint with name meta-textgeneration-llama-2-7b-2024-06-29-08-16-43-090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------!"
     ]
    }
   ],
   "source": [
    "finetuned_predictor = estimator.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb57904a-9631-45fe-bc3f-ae2fbb992960",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluate the pre-trained and fine-tuned model\n",
    "---\n",
    "Next, we use the test data to evaluate the performance of the fine-tuned model and compare it with the pre-trained model. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87085bf6-dc7e-46f3-8563-d2e4aafd0820",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Inputs</th>\n",
       "      <th>Ground Truth</th>\n",
       "      <th>Response from non-finetuned model</th>\n",
       "      <th>Response from fine-tuned model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nAccording to the given paragraph, how do convolutional neural networks avoid overfitting in machine learning?\\n\\n### Input:\\nIn deep learning, a convolutional neural network (CNN) is a class of artificial neural network most commonly applied to analyze visual imagery. CNNs use a mathematical operation called convolution in place of general matrix multiplication in at least one of their layers. They are specifically designed to process pixel data and are used in image recognition and processing. They have applications in image and video recognition, recommender systems, image classification, image segmentation, medical image analysis, natural language processing, brain–computer interfaces, and financial time series.\\n\\nCNNs are also known as Shift Invariant or Space Invariant Artificial Neural Networks (SIANN), based on the shared-weight architecture of the convolution kernels or filters that slide along input features and provide translation-equivariant responses known as feature maps. Counter-intuitively, most convolutional neural networks are not invariant to translation, due to the downsampling operation they apply to the input.\\n\\nCNNs are regularized versions of multilayer perceptrons. Multilayer perceptrons are usually fully connected networks, that is, each neuron in one layer is connected to all neurons in the next layer. The \"full connectivity\" of these networks make them prone to overfitting data. Typical ways of regularization, or preventing overfitting, include: penalizing parameters during training (such as weight decay) or trimming connectivity (skipped connections, dropout, etc.) Developing robust datasets also increases the probability that CNNs will learn the generalized principles that characterize a given dataset rather than the biases of a poorly-populated set.\\n\\n\\n\\n### Response:\\n</td>\n",
       "      <td>A Convolutional Neural Network (CNN) has a fully connected layer in which each neuron is connected to every neuron in the next layer, this structure helps to avoid overfitting</td>\n",
       "      <td>\\n1. A CNN avoids overfitting by making their weights more generalizable via the following approaches:\\n    * 1) It uses random values in its first few layers. (Ex: weights and biases).\\n    * 2) It reduces the number of parameters to optimize.\\n    * (3) It introduces data augmentation techniques to reduce overfitting. For example, by applying translation, rotation, color shifts, grayscale etc.\\n\\n##</td>\n",
       "      <td>In artificial neural networks, multilayer perceptrons are typically fully connected networks. When multilayer perceptrons aren't regularized, they're more inclined to overfit data. Several of the typical methods of regularization include: (weight decay): penalizing parameters in training, or decrementing the connectivity in the network by eliminating connections (dropping out) or using skip connections. Obtaining large, accurate data sets improves the chance that CNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nDid Trump lose the 2020 presidential election?\\n\\n### Input:\\nTrump lost the 2020 presidential election to Joe Biden but refused to concede defeat, falsely claiming widespread electoral fraud and attempting to overturn the results by pressuring government officials, mounting scores of unsuccessful legal challenges, and obstructing the presidential transition. On January 6, 2021, Trump urged his supporters to march to the U.S. Capitol, which many of them then attacked, resulting in multiple deaths and interrupting the electoral vote count.\\n\\n\\n\\n### Response:\\n</td>\n",
       "      <td>Yes. Trump lost the 2020 presidential election, but refused to concede defeat. He furthermore urged his supporters to commit insurrection and attack the U.S. Capitol on January 6th 2021.</td>\n",
       "      <td>Trump did lose the 2020 presidential election to Joe Biden but refused to concede defeat, falsely claiming widespread electoral fraud and attempting to overturn the results by pressuring government officials, mounting scores of unsuccessful legal challenges, and obstructing the presidential transition. On January 6, 2021, Trump urged his supporters to march to the U.S. Capitol, which</td>\n",
       "      <td>Donald Trump lost the 2020 presidential election to Joe Biden, resulting in major demonstrations on the United States Capitol. On January 6, 2021, President Trump asked his supporters to march to the US Capitol. A large group of Trump supporters converged on the U.S. Capitol, resulting in a melee in the Capitol which interrupted the counting of Electoral College votes. Rioters overwhelmed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nTell me about David Wolfenberger\\n\\n### Input:\\nDavid Wolfenberger (born April 19, 1969 in Tuscola, Illinois) is a singer-songwriter from Cincinnati, Ohio. Former frontman for The Marshwiggles and Thom Scarecrow, Wolfenberger has three solo CDs to date; Tales From Thom Scarecrow, and World of the Satisfy'n Place on Blue Jordan Records and more recently in 2006 on Fundamental Records, Portrait of Narcissus. Wolfenberger also toured and recorded as a member of Mark Olson and Victoria Williams' Original Harmony Ridge Creekdippers. Wolfenberger occasionally records under the pseudonym Sunrise for Someone.\\n\\nBiography\\nDave Wolfenberger was part of a band called Selah while a History student at the University of Cincinnati in the early 1990s. This band was later renamed The Remnant due to another band having the same name. They played monthly concerts at a local church. One of their songs, \"I Am Here\" was written by Dave Wolfenberger and helped at least one person, me, accept Jesus to become his personal Savior. This band came out with two tapes of their music. The first album contained the song \"I AM HERE.\" While I have been told there are at least two versions of this song, one version of the song was recently sung by David and can be listened to at Crossroads Church website though the external link I have placed in the external links. The link will be preceded by the number 2.\\n\\nIn 1997 Wolfenberger's band the Marshwiggles released their first and only album Stone Soup on the local Cincinnati label Blue Jordan Records. It was played nationally and the band toured regionally to sizable crowds but broke up during the recording of their second album just prior to its completion and release.[citation needed] This album has never been released although tracks from it have shown up on Blue Jordan compilations. Wolfenberger then formed the band Thom Scarecrow with acoustic guitarist Jason Dennie and fellow Marshwiggles, Tony Moore and Joshua Seurkamp. This would be a short-lived ensemble lasting just over a year.\\n\\nIn 1999 Wolfenberger's first solo album, Tales From Thom Scarecrow was released and won him Artist of the Year in his hometown at the Cincinnati Entertainment Awards as well as notoriety abroad from such notable media as the Corriere della Sera in Milan, Italy which stated that \"Wolfenberger puts forth folk with dark nuances, the grand introspective songs are illuminated with emotions in this exhibition of his life.\"\\n\\nIn 2000 he joined iconoclastic songwriters Mark Olson and Victoria Williams as a touring and recording member of the Original Harmony Ridge Creekdippers. In 2001 while still touring with the Creekdippers Wolfenberger recorded his second solo album with his band entitled World of the Satisfyn' Place. This album was decidedly more roots oriented than his first (a return to his style with the Marshwiggles and Thom Scarecrow) and swung wildly from raucous to thoughtful on a song to song basis., It was even more well received by the critics. The Detroit Metro Times stated that \"Wolfenberger ambles between homespun Appalachian traditions and classic pop and country forms with an unwavering dedication to simple, gorgeous melodies that are alternately uplifting and devastating.\" While in Britain, Americana-UK called it \"a 100% solid gold classic\". and his hometown Cincinnati press named it \"one of the best albums (local or national) of the year\".\\n\\nIn the following two years he would appear on two Creekdipper albums, release a free folk gospel album under the pseudonym Sunrise for Someone and eventually in 2003 stop touring and recording with the Creekdippers although maintaining a close relationship, which would become apparent with the release of his third solo album entitled Portrait of Narcissus in 2006 which featured fellow Creekdippers, Victoria Williams, Joshua Grange (by then a regular member of Dwight Yoakam's band), and fellow independent folkie Michelle Shocked. This would be Wolfenberger's first album to be distributed beyond America and Europe into Asia and Australia due to its release by recently revived Fundamental Records. It would even end up at No. 12 on the independent Euro Americana Chart\\n\\nIn July 2008 Wolfenberger recorded and made available another folk gospel project under the pseudonym Sunrise for Someone entitled Summer Lake Champion. In August of the same year he released Just Burned Upon The Page a live and mostly solo recording of 7 songs. The proceeds benefit the Juvenile Diabetes Research Foundation.\\n\\n\\n\\n### Response:\\n</td>\n",
       "      <td>David Wolfenberger is a singer-songwriter from Cincinnati, Ohio. Dave Wolfenberger was part of a band called Selah while a History student at the University of Cincinnati in the early 1990s. This band was later renamed The Remnant due to another band having the same name. In July 2008 Wolfenberger recorded and made available another folk gospel project under the pseudonym Sunrise for Someone entitled Summer Lake Champion. Wolfenberger has three solo CDs to date; Tales From Thom Scarecrow, and World of the Satisfy'n Place on Blue Jordan Records and more recently in 2006 on Fundamental Records, Portrait of Narcissus.</td>\n",
       "      <td>David Wolfenberger was born in Illinois and moved to Cincinnati when he was in college. He is most famous for his album World of the Satisfyn' Place which came out in 2001. As far as touring goes he was a member of the musical group Thom Scarecrow from 1998 to 2000 and joined the group Original Harmony Ridge Creekdippers in 2000 and still performs with them</td>\n",
       "      <td>David Wolfenberger (born April 19, 1969 in Tuscola, Illinois) is a singer-songwriter from Cincinnati, Ohio. Former frontman for The Marshwiggles and Thom Scarecrow, Wolfenberger has three solo CDs to date; Tales From Thom Scarecrow, and World of the Satisfy'n Place on Blue Jordan Records and more recently in 2006 on Fundamental</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWithout quoting directly from the text give me a summary of the half dome hike.\\n\\n### Input:\\nThe Half Dome Cable Route hike runs from the valley floor to the top of the dome in 8.2 mi (13 km) (via the Mist Trail), with 4,800 ft (1,460 m) of elevation gain. The length and difficulty of the trail used to keep it less crowded than other park trails, but in recent years the trail traffic has grown to as many as 800 people a day. The hike can be done from the valley floor in a single long day, but many people break it up by camping overnight in Little Yosemite Valley. The trail climbs past Vernal Fall and Nevada Fall, then continues into Little Yosemite Valley, then north to the base of the northeast ridge of Half Dome itself.\\n\\nThe final 400 ft (120 m) ascent is steeply up the rock between two steel cables used as handholds. The cables are fixed with bolts in the rock and raised onto a series of metal poles in late May (the poles do not anchor the cables). The cables are taken down from the poles for the winter in early October, but they are still fixed to the rock surface and can be used. The National Park Service recommends against climbing the route when the cables are down or when the surface of the rock is wet and slippery. The Cable Route is rated class 3, while the same face away from the cables is rated class 5.\\n\\n\\nThe Cable Route gets crowded on the weekends\\nThe Cable Route can be crowded. In past years, as many as 1,000 hikers per day have climbed the dome on a summer weekend, and about 50,000 hikers climb it every year.\\n\\nSince 2011, all hikers who intend to ascend the Cable Route must now obtain permits before entering the park when the cables are up between May and October. Permits are checked by a ranger on the trail, and no hikers without permits are allowed to hike beyond the base of the sub-dome or to the bottom of the cables. Hikers caught bypassing the rangers to visit either the sub-dome or main dome without a permit face fines of up to $5,000 and/or 6 months in jail.\\n\\nBackpackers with an appropriate wilderness permit can receive a Half Dome permit when they pick up their wilderness permit with no additional reservation required. Rock climbers who reach the top of Half Dome without entering the subdome area can descend on the Half Dome Trail without a permit.\\n\\nThe top of Half Dome is a large, flat area where climbers can relax and enjoy their accomplishment. The summit offers views of the surrounding areas, including Little Yosemite Valley and the Valley Floor. A notable location to one side of Half Dome is the \"Diving Board\", where Ansel Adams took his photograph Monolith, the Face of Half Dome on April 10, 1927. Often confused with \"the Visor,\" a small overhanging ledge at the summit, the Diving Board is on the shoulder of Half Dome.\\n\\nFrom 1919 when the cables were erected through 2011, there have been seven fatal falls from the cables. The latest fatality occurred on September 5, 2019.\\n\\nLightning strikes can be a risk while on or near the summit. On July 27, 1985, five hikers were struck by lightning, resulting in two fatalities.\\n\\nThe Cable Route was added to the National Register of Historic Places in 2012.\\n\\n\\n\\n### Response:\\n</td>\n",
       "      <td>The half dome hike is 8.2 miles long with 4800 ft elevation gain. This hike can be done in a single long day or in two days by camping overnight in Little Yosemite Valley. The trail goes past Vernal and Nevada waterfalls, continues to Little Yosemite Valley and then to the half dome itself. The final 400 ft of the hike involves a steep ascent up the rock using the two steel cables fixed with bolts in the rock. The cables are put up between May and October and it is not recommended to climb the route when cables are down since the surface can be wet and slippery. All the hikers to half dome are required to obtain permits which are checked by a ranger on the trail. Hikers caught bypassing the rangers without permits are subject to penalties.</td>\n",
       "      <td>The Half Dome Cable Route hike runs from the valley floor to the top of the dome in 8.2 mi (13 km) (via the Mist Trail), with 4,800 ft (1,460 m) of elevation gain. The length and difficulty of the trail used to keep it less crowded than other park trails, but in recent years the trail traffic has grown to as many as 800 people a</td>\n",
       "      <td>The half dome hike goes from a valley floor up half dome summit which is approx 8.2 mi (13 km) up 4,800 ft (1,460 m) in a single hard day or overnight for people who choose to camp. Its about a class 3 climb and is a popular and well established hike among park visitors because its less crowded and because of the beauty and views it provides. It passes by</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat was the first Porsche 911 with AWD?\\n\\n### Input:\\nThe 993 Turbo coupé was introduced in 1995. It featured a new twin-turbocharged engine displacing 3.6 liters and generating a maximum power output of 300 kW (408 PS; 402 hp). Air-to-air intercoolers, electronic engine management, redesigned cylinder heads, and other modified engine internals completed the new engine. The 993 Turbo was the first 911 Turbo with all-wheel drive, taken from the 959 flagship model. The Turbo's bodywork differs from the Carrera by widened rear wheel arches (about 6 cm), redesigned front and rear bumper moldings, and a fixed \"whale tail\" rear wing housing the intercoolers. New 18-inch (460 mm) alloy wheels with hollow spokes were standard.\\n\\n\\n\\n### Response:\\n</td>\n",
       "      <td>The 993 had a Turbo variant that started in 1995. It was the first 911 Turbo with AWD. This AWD system was taken from the 959 flagship model.</td>\n",
       "      <td>**996** - Porsche 996 Turbo Coupe (1999-2001)\\n\\n**993** - Porsche 993 Turbo 4S Coupe (1998-2001)\\n\\n**964** - Porsche 964 Turbo 4S Coupe (1993-1994)\\n\\n**959** - Porsche 9</td>\n",
       "      <td>The 993 Turbo was the first 911 Turbo with all-wheel drive, taken from the 959 flagship model.\\n\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "test_dataset = train_and_test_dataset[\"test\"]\n",
    "\n",
    "inputs, ground_truth_responses, responses_before_finetuning, responses_after_finetuning = (\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    ")\n",
    "\n",
    "\n",
    "def predict_and_print(datapoint):\n",
    "    # For instruction fine-tuning, we insert a special key between input and output\n",
    "    input_output_demarkation_key = \"\\n\\n### Response:\\n\"\n",
    "\n",
    "    payload = {\n",
    "        \"inputs\": template[\"prompt\"].format(\n",
    "            instruction=datapoint[\"instruction\"], context=datapoint[\"context\"]\n",
    "        )\n",
    "        + input_output_demarkation_key,\n",
    "        \"parameters\": {\"max_new_tokens\": 100},\n",
    "    }\n",
    "    inputs.append(payload[\"inputs\"])\n",
    "    ground_truth_responses.append(datapoint[\"response\"])\n",
    "    # Please change the following line to \"accept_eula=True\"\n",
    "    pretrained_response = pretrained_predictor.predict(\n",
    "        payload, custom_attributes=\"accept_eula=True\"\n",
    "    )\n",
    "    responses_before_finetuning.append(pretrained_response[0][\"generation\"])\n",
    "    # Please change the following line to \"accept_eula=True\"\n",
    "    finetuned_response = finetuned_predictor.predict(payload, custom_attributes=\"accept_eula=True\")\n",
    "    responses_after_finetuning.append(finetuned_response[0][\"generation\"])\n",
    "\n",
    "\n",
    "try:\n",
    "    for i, datapoint in enumerate(test_dataset.select(range(5))):\n",
    "        predict_and_print(datapoint)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"Inputs\": inputs,\n",
    "            \"Ground Truth\": ground_truth_responses,\n",
    "            \"Response from non-finetuned model\": responses_before_finetuning,\n",
    "            \"Response from fine-tuned model\": responses_after_finetuning,\n",
    "        }\n",
    "    )\n",
    "    display(HTML(df.to_html()))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b0a0f5-ef34-40db-8ab7-c24a5d14b525",
   "metadata": {},
   "source": [
    "### Clean up resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d73ab2da-d00f-46db-90eb-81812898653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete resources\n",
    "#pretrained_predictor.delete_model()\n",
    "#pretrained_predictor.delete_endpoint()\n",
    "#finetuned_predictor.delete_model()\n",
    "#finetuned_predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
